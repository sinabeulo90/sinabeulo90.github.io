---
layout: post
title:  "DQN을 이용한 자율주행"
date:   2021-02-25 01:00:00 +0900
category: "Grepp/KDT"
tag: "Reinforcement Learning"
plugins: mathjax
---

## DQN을 이용한 자율주행 A2와 Lidar

### 현실에서 학습을 시키는 데에서의 어려움
- 장비 없이 하나의 에피소드가 끝났다는 사실을 DQN에게 알리는게 어렵다.
- 차량에 충둘 감지 장치가 없다면, 충돌 후에도 계속 주행 데이터가 쌓일 가능성이 있다.
- 에피소드가 끝날 때 마다 사람이 일일히 출발 위치에 가져다 놓기가 어렵다.
- 모든 것을 자동으로 알아서 해줄 편한 무언가가 필요하다.

### 시뮬레이터 사용
- 그래서 우리는 Xycar Simulator를 이용하여 그 안에서 학습 시킨 후 자율 주행을 시도할 것이다.
    - 시뮬레이터 사용시 충돌 순간을 감지하여 현재 에피소드를 중지 시킬 수 있다.
    - 또한 사람이 일일히 하지 않아도 자동으로 차량 위치 센서의 상태 등을 초기화 시킬 수 있다.
- 다만, 시뮬레이터와 현실은 엄연한 차이가 있으므로 우리는 시뮬레이터를 현실에 맞도록 수정 작업을 거쳐야 한다.


## Pixel & meter 단위 환산

### 시뮬레이터와 현실과의 간극 메우기
- 시뮬레이터 환경과 현실과 결정적으로 다른 부분은 바로 단위이다.
- 보통은 이런 경우 고정되어 있는 사이즈를 기준으로 시뮬레이터의 단위와 현실 단위 사이의 비율을 계산하고, 시뮬레이터에서 학습된 데이터를 현실에 적용할 때 해당 비율을 사용하는 방법으로 간극을 메울 것이다.

### pixel과 meter 단위 환산
- 차량 규격
    - 가로: 약 0.3m
    - 세로: 약 0.6m
- 시뮬레이터 차량 규격
    - 가로: 64px
    - 세로: 128px
    - 가로: 세로 = 1:2
- px와 m의 비율
    - 차량의 크기는 변하지 않기 때문에 이를 기준으로 비율 측정
    - (차량 길이 m):(이미지 상 차량 길이 px) = (구하고자 하는 길이 m):(구하는 대상이 되는 이미지 길이 px)
    - 0.6m:128px = x m:y px

- 시뮬레이터 규격
    - width = 1000px
    - height = 720px
    - road_width = 220px
- m로 환산
    - 0.6m:128px = (width)m:1000px
        - width = (0.6m * 1000px) / 128px = 4.69m
    - 0.6m:128px = (height)m:1000px
        - height = (0.6m * 720px) / 128px = 3.37m
    - 0.6m:128px = (road_width)m:1000px
        - road_width = (0.6m * 220px) / 128px = 1.03m


### pixel과 meter 단위 환산 - ratio, velocity
- 시뮬레이터에서는 px 단위로, 실제로는 m 단위로 모든 것을 하기 때문에, velocity도 바꿔야 한다.
- Velocity
    - 시뮬레이터의 100px/s를 m/s로 변환한다.
    - 0.6m:128px = x m:y px = x m = 100px
        - xm = 0.469m
    - 100px/s는 0.469m/s로 바꾼다.
- Ratio
    - px와 m의 비율(m ---> px)
    - 0.6m * ratio = 128px
    - ratio = 128px/0.6m = 213.33

### 시뮬레이터와 실제 map 규격
- 시뮬레이터
    - width = 1000px
    - height = 720px
    - road_width = 220px
    - car_width = 64px
    - car_height = 128px
- 실제 map
    - width = 4.69m
    - height = 3.37m
    - road_width = 1.03m
    - car_width = 0.3m
    - car_height = 0.6m


## 환경 구축 

### 작업환경 구축 - 파이썬 패키지 설치
- pytorch v1.2
- pyglet v1.4.11
- visdom v0.1.8.9
- pygame v1.9.6
- dill v0.3.3

### Pytorch
- Facebook 인공지능 연구팀이 개발한 파이썬 머신러닝 라이브러리
- 설치가 쉽고, 소스가 간결하며, 직관적이라는 장점을 지닌다.

### Visdom
- Pytorch에서 사용할 수 있는 시각화 도구 중 하나
- 데이터를 입력하여 그래프를 만들거나 사진, 동영상 등을 올릴 수 있다.

### Pygame
- Python 언어를 사용하여 게임을 만들 수 있는 라이브러리



## Xycar 시뮬레이터를 이용한 DQN 기반 자율주행

### 목표
- 거리 센서 정보를 사용할 수 있는 시뮬레이터에서 강화학습 DQN 기반으로 학습
- 자동차가 벽과 충돌하지 않고, 죽지 않고 살아남도록 만들기

### 수행할 내용
1. main.py 파일의 빈 부분을 xycarRL 패키지를 이용하여 채워 넣기
    - 하이퍼 파라미터(hyper_param) 값을 바꾸고
    - 스테이트(state_select) 값을 바꾸고
2. my_reward.py 파일에 보상 정책에 관한 코드를 작성

- 저장된 .pth파일은 viewer.py 파일을 사용하여 살펴볼 수 있다.


### 디렉토리 구조

```bash
~/
└── DQN 
    ├── main.py
    ├── my_reward.py
    ├── viewer.py
    └── env # 손대지 않기
        ├── __init__.py
        ├── game.py
        ├── model.py
        ├── resource.py
        ├── visual.py
        └── xycarRL.py
```

### main.py 생성
- 중요한 기본 코드가 담겨 있는 파일
    - main.py 파일에서 주석 표시된 부분을 채워 넣어야 한다.
    - 채워 넣을 함수는 제공한 xycarRL.py 파일 안에 있다.

```bash
$ vi main.py
$ chmod +x main.py
```


### main.py 소스코드
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import my_reward

from functools import partial
from env.xycarRL import *
from env.visual import *

if __name__ == "__main__":
    xycar = learning_xycar()
    xycar.set_map("snake")  # snake, square

    # 사람이 정해주는 파라미터
    hyper_param = {
        "sensor_num": 0.0,       # lidar 정보를 몇개 볼 것인가
        "learning_rate": 0.0,
        "discount_factor": 0.0,
        "optimizer_steps": 0,
        "batch_size": 0,
        "min_history": 0,
        "buffer_limit": 0,
        "max_episode": 0,
        "update_cycle": 0,
        "hidden_size": []
    }

    xycar.set_hyperparam(hyper_param)

    # 학습 모델의 input 노드의 수를 결정하는 변수
    state_select = {
        "car sensor": True,
        "car yaw": False,
        "car position": False,
        "car steer": True
    }

    xycar.state_setup(state_select)
    xycar.Experience_replay_init()
    xycar.ML_init("DQN")    # DQN, DDQN, Duel DQN

    visual = visualize(port=8888)
    visual.chart_init()

    ALL_STEPS, ALL_REWARD = 0, 0
    episode = 0

    while 0 <= episoe <= int(hyper_param["max_episode"]):
        episode += 1
        epsilon = max(0.01, 0.8 - 0.01*(episode/200.0))
        xycar.set_E_greedy_func(epsilon)

        # TODO: 환경 초기화
        # ...
        
        reward, score = 0, 0.0

        while not xycar.get_episode_done():
            # TODO: state 값으로 action 값 받아오기
            # ...

            # TODO: 다음 state 값 받아오기
            # ...

            # TODO: episode 종료 여부 확인하기
            # if ...

                # 에피소드 종료시 지급할 reward 설계에 필요한 인자 넣기
                for_reward = [
                    xycar.get_episode_total_time(),
                    xycar.get_sensor_value(),
                    xycar.get_xycar_position(),
                    xycar.get_xycar_yaw()
                    xycar.get_xycar_steering(),
                    xycar.get_step_number(),
                    xycar.get_round_count()
                ]
                reward += my_reward.reward_end_game(for_reward)

                # TODO: Experience_replay 메모리에 state, action, next_state, reward 순으로 넣기
                # ...
                
                break
            
            # 에피소드 중 지급할 reward 설계에 필요한 인자 넣기
            for_reward = [
                xycar.get_episode_total_time(),
                xycar.get_sensor_value(),
                xycar.get_xycar_position(),
                xycar.get_xycar_yaw()
                xycar.get_xycar_steering(),
                xycar.get_step_number(),
                xycar.get_round_count()
            ]
            reward = my_reward.reward_in_game(for_reward)

            
            # TODO: Experience_replay 메모리에 state, action, next_state, reward 순으로 넣기
            # ...

            state = next_state
            score += reward
        
        ALL_STEPS += xycar.get_step_number()
        ALL_REWARD += score

        if xycar.get_max_score() < score:
            xycar.max_score_update(score)
            xycar.model_save(episode)
            xycar.making_video(episode)
        
        if xycar.get_memory_size() > hyper_param["min_history"]:
            # TODO: 훈련 개시 함수
            # ...

            visual.loss_graph_update(episode, loss)
        
        visual.dead_position_update(xycar.get_xycar_position())
        visual.reward_update(episode, score)
        visual.learning_curve_update(episode, ALL_REWARD)

        if xycar.get_memory_size() > hyper_param["min_history"] \
        and (episode % hyper_param["update_cycle"]) == 0 and episode != 0:
            # TODO: main Q를 target Q에 update하기
            # ...
        
        if episode % 10 == 0 and episode != 0:
            print("episode: {}, memory size: {}, epsilon: {:.1f}%".format(
                episode, xycar.get_memory_size(), epsilon * 100))
```

### my_reward.py 생성
- 강화학습은 보상을 언제 어떻게 얼마나 주느냐에 따라서 다양한 형태를 보이는데, 이 파일로 보상 정책을 세울 수 있다.
- for_reward 리스트 안에 필요한 데이터를 넣어 온 후, 원하는대로 리워드를 설계

```bash
$ vi my_reward.py
$ chmod +x my_reward.py
```

### my_reward.py 소스 코드 예시
```python
""" case 1 """
def reward_in_game(data):
    reward = 0
    # YOUR REWARDS
    sensor_value = data[1]
    sensor_value.sort()
    if sensor_value[0] > 20:
        reward = 1.0
    return reward

""" case 2 """
def reward_in_game(data):
    reward = 0
    # YOUR REWARDS
    sensor_value = data[6]
    sensor_value.sort()
    if sensor_value[0] > 20:
        reward = 100.0
    return reward
```

- 필요한 데이터를 넣은 뒤 리워드 식을 제작할 수 있다.
- 지금은 게임 중에, 센서 데이터 중 제일 먼 데이터가 20일 때 1을 준다.
- 그리고 게임 후에 몇 바퀴 돌았는지 체크하여 0바퀴 이상 돌았으면 100을 준다.


### viewer.py 사용법
- pth 파일의 시험 주행을 위한 프로그램
- pth 파일은 학습된 데이터의 저장 파일 확장자이다.
- viewer.py를 아래와 같이 살짝 수정하는 것으로 학습한 pth파일을 테스트 해볼 수 있다.
    1. hidden_layer = [1, 2]
        - 학습 시에 사용한 main.py의 hidden_size 파라미터와 똑같이 입력
    2. lidar_cnt = 5
        - 학습 시에 사용한 센서의 개수를 입력
    3. state_select = ...
        - 학습 시에 사용한 데이터의 종류를 체크
        - 가능한 데이터 종류
            - car sensor: 거리 센서
            - car yaw: 자동차 방향
            - car position: 자동차 좌표
            - car steer: 자동차 조향값
    4. xycar.ML_init("Duel DQN")
        - 학습 시에 사용한 모델의 종류를 기입
        - 기입 가능한 종류
            - "DQN"
            - "DDQN"
            - "Duel DQN"
    5. view_epi = 404
        - 저장된 학습 파일의 에피소드 넘버를 입력(파일 이름 끝이 적혀있는 숫자)
        - ex. main_model_000404.pth ---> '404' 입력
- 모두 수정 후, 다음 명령어를 실행하면 기입된 pth 파일을 불러와 로드한다.

```bash
$ python viewer.py
```



## XycarRL Package
- xycarRL.py 파일은 main.py에서 사용하는 라이브러리
- xycarRL.so 파일(패키지)에는 환경 초기화, 환경 정보 수신, 학습 등 시뮬레이터 환경에서 강화학습을 구동하기 위해 필요한 모든 게 들어있다.

### xycarRL 함수 설명
- xycar.set_map( string )
    - 반환 값: void
    - 맵을 설정하는 함수
        - "square": 정사각형 맵
        - "snake": 꼬불꼬불한 맵
- xycar.get_max_score()
    - 반환 값: float
    - 지금까지 최고 점수를 리턴하는 함수
    - 자동 갱신이 되지 않으므로 max_score_update와 함께 사용하는 것이 좋다.
- xycar.max_score_update( float )
    - 반환 값: void
    - Max score를 저장하는 변수에 입력된 파라미터 값을 저장한다.
- xycar.set_frame( float )
    - 반환 값: void
    - viewer.py 사용시 fps = (frame/sec)를 설정하는 함수
- xycar.set_hyperparam( dict )
    - 반환 값: void
    - 하이퍼파라미터를 설정하는 함수
    - 입력되는 dictionary의 내용은 다음과 같다.
        - sensor_num
        - learning_rate
        - discount_factor
        - optimizer_steps
        - hidden_size
        - batch_size
        - min_history
        - buffer_limit
        - max_episode
        - update_cycle
- xycar.state_setup( dict )
    - 반환 값: void
    - input_size를 결정하는 변수
    - 해당 변수의 key 값은 다음과 같다.
        - car sensor
        - car yaw
        - car steer
- xycar.ML_init( string )
    - 반환 값: void
    - 학습시 사용할 모델을 정할 수 있다.
    - 지정 가능한 모델
        - DQN
        - DDQN
        - Duel DQN
- xycar.Experience_replay_init()
    - 반환 값: void
    - Experience_replay 버퍼를 초기화 하는 함수
- xycar.Experience_replay_memory_input( np.ndarray, int, np.ndarray, float )
    - 반환 값: None
    - Experience_replay memory에 state, action, next state, reward 값을 저정한다.
- xycar.Experience_replay_close()
    - 반환 값: void
    - Experience_replay 버퍼를 닫는 함수
- xycar.pygame_exit_chk()
    - 반환 값: bool
    - pygame이 종료되었는지 확인하는 함수
    - 종료되었다면 True를 반환한다.
- xycar.set_E_greedy_func( float )
    - 반환 값: void
    - 추가 모험의 수행을 결정하는 기준 숫자를 설정하는 함수
- xycar.get_action( np.ndarray )
    - 반환 값: int
    - 상태값을 입력하여 어떤 행동을 취할지 결정하는 함수
- xycar.step( int )
    - 반환 값: np.ndarray
    - 자동차 환경을 한 스텝만큼 진행시키는 함수
    - 입력된 action 값으로 인해 변화된 상태값을 반환한다.
- xycar.set_hidden_size( list )
    - 반환 값: void
    - 학습시 사용할 네트워크의 히든 레이어를 설정한느 함수
- xycar.train( int )
    - 반환 값: void
    - Experience replay에 쌓여 있는 데이터를 바탕으로 학습하는 함수
    - 입력값은 optimizer의 매개변수를 갱신할 횟수(hyperparameter의 optimizer_step)
- xycar.mainQ2targetQ()
    - 반환 값: void
    - 지금까지 학습된 target-Q를 main-Q로 갱신하는 함수
- xycar.set_init_location_pose_random( bool )
    - 반환 값: void
    - 차량의 초기 위치를 랜덤하게 시작하도록 할 것인지, 고정된 위치에서 시작할 것인지 결정
    - True일 경우, 차량의 초기 위치가 랜덤하다.
- xycar.get_episode_total_time()
    - 반환 값: float
    - Episode 시작부터 이 함수를 호출할 때까지의 시간을 반환하는 함수
- xycar.get_sensor_value()
    - 반환 값: list
    - 관측된 라이다 센서의 값을 반환하는 함수
    - ex. [0, 1, 2, 3, 4]
- xycar.get_xycar_position()
    - 반환 값: list
    - 자동차의 현재 위치를 반환하는 함수
    - [x좌표, y좌표] 형식으로 저장되어 있다.
- xycar.get_xycar_yaw()
    - 반환 값: float
    - 자동차의 현재 방향을 반환하는 함수
- xycar.get_xycar_steering()
    - 반환 값: float
    - 자동차의 현재 조향값을 반환하는 함수
- xycar.get_step_number()
    - 반환 값: int
    - 진행 중인 episode에서 현재 처리 중인 step의 번호를 반환하는 함수
- xycar.get_episode_done()
    - 반환 값: bool
    - 현재 에피소드의 종료 여부를 반환하는 함수
- xycar.get_round_count()
    - 반환 값: int
    - 자동차가 몇 바퀴 돌았는지 반환하는 함수
- xycar.get_memory_size()
    - 반환 값: int
    - Experience_replay에 저장되어 있는 데이터의 크기를 반환하는 함수
- xycar.set_lidar_cnt( int )
    - 반환 값: void
    - 거리 데이터를 몇 개 사용할 것인지 결정하는 함수
    - 자동으로 개수를 선택하면, 정면 180도 안에서 일정한 간격으로 입력한 파라미터 값 만큼 데이터를 뽑아준다.


## DQN2Xycar 패키지 제작
- DQN Viewer.py를 이용하여 차량에서 구동하는 ROS 패키지 만들기

### 패키지 설게
- ROSModule.py
    - Ros기능들만 모아 클래스 제작
    - 라이다 --- 라이다 데이터 ---> DQN
- viewer.py
    - 기존의 viewer.py 파일을 변형
    - DQN --- 모터 데이터 ---> VESC


### 패키지 생성
```bash
$ catkin_create_pkg dqn2xycar rospy
```

```bash
./
├── CMakeLists.txt
├── package.xml
└── src
    └── env # 손대지 않기
    │   ├── __init__.py
    │   ├── game.py
    │   ├── model.py
    │   ├── resource.py
    │   ├── visual.py
    │   └── xycarRL.py
    ├── rosModule.py
    ├── viewer.py
    └── save
        └── ...
```


### rosModule.py
- ROS 기능을 합쳐 모아놓은 클래스 제작

```python
#!/usr/bin/env python

import rospy
from sensor_msgs.msg import LaserScan
from ackermann_msgs.msg import AckermannDriveStamped

class rosmodule:
    
    laser_msg = None
    ack_msg = AckermannDriveStamped()
    ack_msg.header.frame_id = "odom"


    def __init__(self):
        rospy.init_node("dqn2xycar", anonymous=True)
        self.launch_data_read()

        rospy.Subscriber("/scan", LaserScan, self.lidar_callback)
        self.ackerm_publisher = rospy.Publisher("ackermann_cmd", AckermannDriveStamped, queue_size=1)


    def auto_drive(self, steer_val, car_run_speed):
        self.ack_msg.header.stamp = rospy.Time.now()
        self.ack_msg.drive.steering_angle = steer_val
        self.ack_msg.drive.speed = car_run_speed
        self.ackerm_publisher.publish(self.ack_msg)
    
    
    def lidar_callback(self, data):
        self.laser_msg = data.ranges
    
    
    def launch_data_read(self):
        self.hidden_size = []

        hidden_size_str = rospy.get_param("~hidden_size", "[]")
        self.view_epi = rospy.get_param("~view_epi", "0")
        self.output_size = rospy.get_param("~output_size", 0)
        self.LoadPath_main = rospy.get_param("~loadPath", "")

        hidden_size_str_list = hidden_size_str.replace("[", "").replace("]", "").split(",")
        for i in hidden_size_str_list:
            self.hidden_size.append(int(i))
    
    
    def get_laser_msg(self):
        return self.laser_msg
    

    def get_view_epi(self):
        return self.view_epi
    

    def get_output_size(self):
        return self.output_size
    

    def get_pth_path(self):
        return self.LoadPath_main
    

    def get_hidden_size(self):
        return self.hidden_size


    def get_ros_shutdown_chk(self):
        return not rospy.is_shutdown()
```


### viewer.py - DQN 모듈

```python
#!/usr/bin/env python
import time
from env.xycarRL import *
from rosModule import *

def next_state_rtn(laser_msg):
    ratio = 213.33
    idx = [0, 44, 89, 134, 179]
    current_ipt = []

    for i in range(len(idx)):
        if idx[i] == 0:
            tmp = [laser_msg[idx[i]], laser_msg[idx[i]+1], laser_msg[idx[i]+2]]
        elif idx[i] == 179:
            tmp = [laser_msg[idx[i]-2], laser_msg[idx[i]-1], laser_msg[idx[i]]]
        else:
            tmp = [laser_msg[idx[i]-1], laser_msg[idx[i]], laser_msg[idx[i]+1]]
        
        current_ipt.append(min(tmp))
    
    rtn = np.array(current_ipt)

    for j in range(len(curent_ipt)):
        rtn[j] *= ratio
    return rtn

if __name__ == "__main__":
    xycar = learning_xycar(False)

    hidden_layer = [256, 256]
    lidar_cnt = 5
    xycar.set_lidar_cnt(lidar_cnt)
    xycar.set_hidden_size(hidden_layer)

    ros_module = rosmodule()

    state_select = {
        "car sensor": True,
        "car yaw": False,
        "car position": False,
        "car steer": True
    }

    xycar.state_setup(state_select)
    xycar.screen_init()
    xycar.ML_init("DQN")

    view_epi = 0
    xycar.load_model(view_epi)

    time.sleep(0.5)

    angle = 0
    max_angle = np.radians(30.0)
    handle_weights = np.radians(6.6)
    rate = rospy.Rate(30)

    state = xycar.episode_init()

    while ros_module.get_ros_shutdown_chk():
        action = xycar.get_action_viewer(state)
        if action == 2:
            angle += handle_weights
        elif action == 0:
            angle -= hangle_weights
        elif action == 1:
            angle = 0
        
        angle = max(-max_angle, min(angle, max_angle))
        ros_module.auto_drive(angle, 0.234)

        next_state = next_state_rtn(ros_module.get_laser_msg())

        state = next_state
        rate.sleep()
```


## DQN 학습 Weight 적용

### 학습 weights을 차량에 적용
- 적용할 weight 파일을 `dqn2xycar/src/save` 폴더에 옮기기
- Python 코드(viewer.py) 상의 view_epi를 weight 파일 번호로 변경
    - ex. weight 파일: main_model_012345.pth ---> view_epi: 12345
- Python 코드(viewer.py) hidden_layer를 학습시 사용한 노드 개수를 입력
- Python 코드(viewer.py) lidar_cnt를 학습시 사용한 거리측정 장치 개수를 입력


### 실패 원인 가정
- 앞서 만든 weight 파일을 차에 적용시켜 주행 ---> 실패
- Why?
    - 라이다가 측정할 수 있는 거리는 최대 값이 존재함
    - 그래서 최대 값을 널머가는 거리일 때, inf 값이 들어옴
    - 또한, 너무 거리가 가까울 때에도 거리를 측정하지 못해 inf 값이 들어옴
- 그렇다면 학습할 때도 적정량의 inf 값을 주면 제대로 학습되지 않을까?


### inf 값을 줘서 시뮬레이터 학습
- 주행 중 inf 값이 어느 정도나 나오는지 측정
    - 실제 라이다의 1frame에 들어오는 라이더 데이터 중 inf의 비율을 측정
    - 측정시 180개의 데이터 중 30~40개 정도 inf 값이 들어옴을 확인
- 시뮬레이터에서 라이다의 데이터를 30~40개 정도 랜덤으로 선택하여 inf 값으로 변경
- 그 후 학습 ---> 실패
- Why?
    - inf 값은 무한대의 수, Loss 계산 시 터무니 없는 값이 나오는 경우가 생김
    - 따라서 학습하기 곤란함


### inf를 라이다의 최대 값으로 변경
- 그렇다면 inf를 유한하게 만들면 어떨까?
- 라이더의 최대 측정 거리는 14m로 이를 px 단위로 변경
    - 1m = 약 213.33px
    - 14m = 약 2986.62px
- 시뮬레이터에서 라이다의 데이터를 30~40개 정도 랜덤으로 선택하여 2986px 값으로 변형
- 그 후 학습 ---> 실패
- Why?
    - inf 값과 같이 Loss 값이 터무니 없는 숫자가 출력
    - 그러나 현실에서는 너무 멀거나 너무 가까울 때도 inf 값이 나옴
    - 그리고 주변 환경의 영향(빛 반사, 습도 등)으로 inf 값이 나오기도 함
    - 따라서 학습하기 곤란함


### inf 주변 값 평균
- 지금까지는 학습시 시뮬레이터를 현실에 맞추려고 했지만, 역으로 현실의 데이터를 시뮬레이터답게 바꿔서 학습하는 방법을 사용해보자.
- 시뮬레이터에서는 무한대 값이 존재하지 않으며, 튀는 값이 없는 거리 데이터 구조를 유지한다.
- 그래서 우리는 앞으로 라이다 데이터를 수신했을 시, 차의 입장에서는 장애물이 가까울수록 굉장히 불리해지므로, 해당 각도의 좌/우의 거리값을 비교하여 가장 작은 inf 대신 넣는 방법을 사용한다.


### inf 주변 값 중 최소값
- 라이다의 값
    - 360도 거리가 배열의 형태(인덱스 0~359)로 들어옴
    - 우리가 사용할 범위는 0~180 ---> 뒤쪽은 차량의 방해를 받고, 필요가 없음
- 인덱스가 0일 때,
    - 0보다 앞선 인덱스(-1)가 없기 때문에 인덱스 0보다 큰 값들을 사용
    - 인덱스 1의 값과 2의 값으로 최소값을 구함
- 인덱스가 180일 때,
    - 인덱스 180의 뒤에 값(181)이 없기 때문에, 인덱스 180보다 작은 값들을 사용
    - 인덱스 179의 값과 178의 값으로 최소값을 구함


### inf 주변 값 중 최소값
- `current_ipt[i] == float("inf")`
- `if idx[i] == 0:`
    - 인덱스가 0인 laser_msg의 경우엔 자기 자신과 1과 2를 이용하여 최소값을 구함
- `elif idx[i] == 179:`
    - 인덱스가 179인 laser_msg의 경우엔 자기 자신과 178과 177를 이용하여 최소값을 구함
- `else:`
    - 인덱스가 0과 179가 아닌 경우엔 양 옆의 수와 비교하여 세개 중 최소값을 구함


### inf 주변 값 중 최소값
- 인덱스 359, 181을 사용하지 않는 이유
    - inf 가 반복될 때, 차량의 방해를 받는 값을 가져올 수 있고, 조금 더 간결한 수식을 위해
- inf를 주변 값 중 가장 작은 값을 줘서 주행 ---> 성공
- Problem
    - 제대로 주행은 하지만 코너를 돌 때, 코너와 근접하게 붙어서 주행
    - 따라서 차량이 코너에 걸리는 상황 발생
    - 차가 중심을 맞춰서 주행할 수 있도록 학습시키자


### 중심을 맞춰서 주행
- 중심을 맞추기 위해서는 어떻게 학습 리워드 알고리즘을 짜야 할까?
    - 라디아 값으로 계산 방향과 지금 주행하고 있는 방향을 비교해서, 그 차이를 0과 1사이의 값으로 환산해서 reward를 주자!
- 고려해야 할 부분
    - 기계는 사람이 아니므로 리워드 함수가 학습 중에 계속 바뀌는 값을 도출해내면, 학습할 때 에이전트 입장에서 굉장히 혼란스러울 수 있음
    - 따라서, func(input value) = reward 형식으로 짜야 함

### 중심을 맞춰서 주행 - 현재 차량의 방향
- 구해야 하는 값: 현재 방향을 나타내는 각도
    - 방향은 벡터 값이므로, 라이다 값들을 사용해서 벡터(방향)를 계산해야 함
    - 현재 바퀴의 각도 ---> 현재 방향: 차량의 조향 값 $\theta$
        - 차량 조향 값의 최소(-30), 최대(30)
        - 왼쪽(+), 직진(0), 오른쪽(-)


### 중심을 맞춰서 주행 - 라이다 값의 방향
- 구해야 하는 값: 라이다 값으로 계산된 각도
    - 라이다의 값들을 사용해 벡터의 평균을 구해 나온 각도 ---> 적절한 방향
        - x성분: $\sin\theta \times \text{거리}$
        - y성분: $\cos\theta \times \text{거리}$
        - $\sin\theta$: x성분 / 거리
        - $\cos\theta$: y성분 / 거리
        - 왼쪽(인덱스 180), 오른쪽(인덱스 0)
        - 라이다 값은 5개를 사용하기 때문에, 각 성분 5개의 평균을 구해서 사용
            - x성분 평균: $\sum x$ / 5
            - y성분 평균: $\sum y$ / 5
        - 평균 벡터 거리
            - 위에서 구한 x성분의 평균과 y성분의 평균을 사용해서 구해지는 거리
            - 평균 벡터 거리: $\sqrt{(x\text{ 성분 평균})^2 + (y \text{ 성분 평균})^2}$
    - 각도 순서: 반시계 방향
    - 바퀴 조향의 부호에 맞게 조정
- 지금까지 구해진 값
    - x성분 평균
    - y성분 평균
    - 평균 벡터 거리
- 이제 이 값들로 라이다 값의 방향 $\theta$를 계산해보자.
    - 각도 $\theta$를 구하는 방법
        -  $\theta = \arcsin(x \text{ 성분 평균 / 평균 벡터 거리})$
        -  $\theta = \arccos(y \text{ 성분 평균 / 평균 벡터 거리})$
    - 우리는 좌측은 양수, 우측은 음수인 값을 도출해야 하므로, 위 공식 중 $\arcsin$을 사용해야 한다.
        - $\sin$: 좌측(+)/우측(-)
        - $\cos$: 좌측(+)/우측(+)
- 위의 공식과 우리가 라이더로부터 구한 값들을 사용해서 각도 $\theta$를 구하자.
    - $\theta = \arcsin(x \text{ 성분 평균 / 평균 벡터 거리})$
- 구해진 $\theta$는 Radians 단위이고, 차량의 방향은 Degrees 단위를 사용하므로, $\theta$를 Degrees로 변환해준다.
- 구해진 각도가 -30~30의 범위를 벗어나면 차량 방향의 최대/최소 값으로 세팅한다.


### 중심을 맞춰서 주행 - 두 값 비교하기
- 두 값의 차: 현재 차량의 방향 - 라이다 값으로 구해진 방향
- 리워드는 0에서 1사이의 값으로 나와야 하므로, 두 값의 차를 0~1 사이의 값으로 변환해야 함
- 0과 1 사이의 값으로 변환
    - 두 값의 최소/최대 값은 -30/30이므로 총 60
    - 따라서 두 값의 차는 -60부터 60까지의 값이 나온다.
    - 두 값의 차를 절대값으로 변환하면, 0부터 60까지의 값이 나온다.
    - 절대값으로 변환한 두 값의 차를 60으로 나누면 0과 1사이의 값이 나온다.
- Reward = \| 라이다 벡터 평균의 사이각 - 스티어링 값 \| / 60


### 중심을 맞춰서 주행 - 과제(my_reward.py)
- `reward_in_game(data1, data2)`
    - 주행 진행시 주는 리워드 함수
    - data1: 라이다 값
    - data2: 조향 값(steering)
    - reward_cal: 중심 맞춰 주행하는 것에 대한 리워드 계산 함수
- `reward_end_game(data)`
    - 주행 종료 시 주는 리워드 함수

```python
def reward_in_game(data1, data2):
    reward = 0.5

    # YOUR REWARDS
    sensor_value = data1
    steering = data2
    sensor_value.sort()

    reward += reward_cal(sensor_value, steering)
    return float(reward)


def reward_end_game(data):
    reward = 0

    # YOUR REWARDS
    round_count = data

    return float(reward)
```


### 중심을 맞춰서 주행 - state에 steering 추가
- state에 steering 추가
    - steering 값을 reward 계산에 사용하기 때문에, 이를 state에 추가해줘야 함
    - stae에 없는 값을 넣어 reward를 부여하면, 그 값이 state가 아닌 외부 요인으로 작용하여 노이즈와 같이 작용됨
    - 따라서 reward 계산에 필요한 값은 꼭 state에 추가해야 함
- state에 steering 추가하지 않고 학습했을 때
    - ...
- 앞에서 만들어진 reward 함수로 학습 진행 ---> 제대로 안됨
- 주행하는 모습을 보면, 처음엔 우회전으로 잘 돌지만 두번째 좌회전을 할 때도 계속 우회전 방향으로 주행함
- Why?
    - 학습 패턴을 보면, E-greedy 함수대로 초반부 코너의 우회전은 학습이 잘 됨
    - 후반부의 좌회전부터는 모험을 거의 안해서 학습이 잘 안되는 것으로 보임

### 랜덤한 주행 시작 위치
- 후반부 학습이 잘 안되는 문제 해결 방법
    - 그렇다면 주행 시작 위치를 랜덤하게 주면 여러 방향으로 학습할 수 있지 않을까?
    - 시작 위치가 바뀌면서 다양한 커브를 학습해 균형적인 학습이 가능해짐
- set_init_location_pose_random( bool ) 함수 참조
    - 해당 함수 사용시 인자에 False를 넣으면, 특정 위치에서만 시작
    - True를 넣으면 랜덤으로 위치가 바뀌어서 시작


### DDQN 학습 그래프
- reward value: 중간중간 높은 reward를 낸다.
- death position: 다양한 위치에서 죽으며 다양한 방향과 커브를 학습한다.
- Learning Curve: 계속 올라가는 형태의 그래프
- Loss Graph: 극초반엔 높은 loss를 내지만, 후반부엔 낮은 loss를 냄


### DDQN 학습 weight 선택
- 학습 후 나온 video를 보면서 적절한 학습 weight를 선택
- 우리의 목적은 중심을 맞추려고 노력함으로써 벽에 닿지 않고 주행하는 것
- 해당 시뮬레이터 사용시 의도대로 조금씩 띄우려고 하다가 어느 순간부터 다시 붙어가는 것은 확인 할 수 있음
- 우리가 필요한 것은 살짝 떨어진 것이므로, 너무 많은 학습은 필요하지 않다는 것을 알 수 있음
- 따라서 2000번대 학습 파일을 사용하면 적당할 것으로 보임


### DQN과 DDQN
- DQN의 문제
    - Loss 그래프가 잘 안 줄어듬
    - 일반적인 딥러닝에서는 줄어들어야 하지만, DQN에서는 잘 줄어들지 않음
    - 이러한 DQN의 문제를 해결한 것이 DDQN
- DQN과 DDQN 코드(xycarRL.py)
    - DQN: `max_q_prime = self.q_target(next_state).max(1)[0].unsqueeze(1)`
    - DDQN: `max_q_prime = self.q_target(next_state).detach().gather(1, self.q(next_state).detach().max(1)[1].unsqueeze(1))`
- DQN loss graph
- DDQN loss graph
