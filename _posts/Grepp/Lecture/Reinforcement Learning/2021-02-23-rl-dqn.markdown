---
layout: post
title:  "DQN"
date:   2021-02-23 02:00:00 +0900
category: "Grepp/KDT"
tag: "Reinforcement Learning"
plugins: mathjax
---

## DQN
- CNN + Q-Learning
    - Q-Learning과 인공신경망을 결함
    - 이미지 정보를 입력으로 하는 경우 CNN 사용
    - 벡터 정보를 입력으로 하는 경우 ANN 사용
- 특별한 규칙에 대한 학습 없이 게임 화면만을 통해 학습
- Input: 게임 화면 / Output: 각 action에 대한 value


### Update
- Target
    - 다음 state가 terminal인 경우: Reward(현재 state)
    - 다음 state가 terminal이 아닌 경우: Reward(현재 state) + $\gamma$ * Max Q(다음 state)
- Loss: $(\text{Target} - Q)^2$ 을 이용하여 최적화 수행
    - Learning rate $\alpha$를 이용하여 학습
    - 목표값과 현재 state와 action에 대해 계산된 Q-value의 차이가 최소가 되도록 학습
    - 학습이 진행될수록 네트워크는 목표값과 가까운 Q-value를 구하게 됨


### DQN 코드
- Environment: CartPole
    - gym에서 제공하는 환경 중 하나
    - 카트를 좌우로 이동하여 막대가 쓰러지지 않도록 제어
    - 상태(4개)
        - 카트의 위치
        - 카트의 속력
        - 막대기의 각도
        - 막대기의 끝부부(상단) 속도
    - 행동(2개)
        - 카트를 좌/우로 이동
    - 보상
        - 막대기가 넘어지지 않으면 +1점을 받음
    - 게임 종료
        - 막대기가 수직으로부터 12도 이상 기울어짐(-12도 ~ 12도)
        - 카트가 중심으로부터 2.4이상 벋어남(-2.4 ~ 2.4)
        - 시간 스텝이 200보다 커짐

```python
import numpy as np
import random
import datetime, os     # 학습된 모델을 저장하기 위해 사용
import gym

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn

# Environment
env = gym.make("CartPole-v0")

# Parameter Setting
algorithm = "DQN"

state_size = 4                      # 상태의 수 정의
action_size = env.action_space.n     # 행동의 수 정의

load_model = False      # 학습된 모델을 불러올지 결정
train_mode = True       # 학습을 수행할지 결정

batch_size = 32

discount_factor = 0.99
learning_rate = 0.00025

run_step = 40000        # 학습을 진행할 스텝
test_step = 10000       # 학습 후 테스트를 진행할 스텝

print_episode = 10      # 해당 에피소드마다 한번씩 진행 상황 출력
save_step = 20000       # 해당 스텝마다 네트워크 모델 저장

epsilon_init = 1.0      # 초기 epsilon
epsilon_min = 0.1       # epsilon의 최소값

date_time = datetime.datetime.now().strftime("%Y%m%d-%H-%M-%S")

save_path = "./saved_models/" + date_time
load_path = "./saved_models/20210205-18-52-50_DQN"

# 딥러닝 연산을 위한 device 결정
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



# 딥러닝 모델
class DQN(nn.Module):

    def __init__(self):
        super(DQN, self).__init__()
        # 네트워크 모델: 3층의 은닉층으로 구성된 인공신경망
        self.fc1 = nn.Linear(state_size, 512)
        self.fc2 = nn.Linear(512, 512)
        self.fc3 = nn.Linear(512, action_size)
    

    def forward(self, x):
        # 입력: 상태
        # 출력: 각 행동에 대한 Q 값
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x



# DQNAgent 클래스: DQN 알고리즘을 위한 다양한 함수 정의
class DQNAgent():

    def __init__(self, model, optimizer):
        # 클래스의 함수들을 위한 값 설정
        self.model = model          # 네트워크 모델 정의
        self.optimizer = optimizer  # 최적화기 정의
        
        self.epsilon = epsilon_init
    
        if load_model == True:
            self.model.load_state_dict(torch.load(load_path+"/model.pth"), map_location=device)
            print("Model is loaded from {}".format(load_path+"/model.pth"))
    

    # Epsilon greedy 기법에 따라 행동 결정
    def get_action(self, state):
        if train_mode:
            if self.epsilon > np.random.rand():
                # 랜덤하게 행동 결정
                return np.random.randint(0, action_size)
        
        with torch.no_grad():
            # 네트워크 연산에 따라 행동 결정
            Q = self.model(torch.FloatTensor(state).unsqueeze(0).to(device))
            return np.argmax(Q.cpu().detach().numpy())


    # 네트워크 모델 저장
    def save_model(self, load_model, train_mode):
        # 처음 모델을 학습하는 경우, 폴더 생성 및 save_path에 모델 저장
        if not load_model and train_mode:   # first training
            os.makedirs(save_path + algorithm, exist_ok=True)
            torch.save(self.model.state_dict(), save_path + algorithm + "/model.pth")
            print("Save Model: {}".format(save_path + algorithm))
        # 기존에 학습된 모델을 불러온 뒤 이어서 학습하는 경우, load_path에 모델 저장
        elif load_model and train_mode:     # additional training
            torch.save(self.model.state_dict(), load_path + "/model.pth")
            print("Save Model: {}".format(load_path))

    
    # 학습 수행을 진행하는 train_model 함수
    def train_model(self, state, action, reward, next_state, done):
        # 상태, 행동, 보상, 다음 상태, 게임 종료 정보를 입력으로 받음

        # 인공신경망 연산을 위해 현재 상태와 다음 상태를 torch의 Tensor로 변환 후 device에 올림
        state = torch.Tensor(state).to(device)
        next_state = torch.Tensor(next_state).to(device)

        # 예측된 Q값 중 에이전트가 취한 행동에 대한 Q값 도출(예측값)
        one_hot_action = torch.zeros(2).to(device)
        one_hot_action[action] = 1
        q = (self.model(state) * one_hot_action).sum()

        # 타겟의 식에 따라 타겟값 계산
        # Done = True: target_q = reward
        # Done = False: target_1 = reward + discount factor * max(nextQ)
        with torch.no_grad():
            max_Q = q.item()
            next_q = self.model(next_state)
            target_q = reward + next_q.max()*(discount_factor*(1 - done))
        
        # 예측값(q)와 타겟값(target_q) 사이의 손실 함수값 계산(smooth L1 Loss)
        loss = F.smooth_l1_loss(q, target_q)

        # 인공신경망 학습
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # loss와 Q값 반환
        return loss.item(), max_Q


# 메인 함수
if __name__ == "__main__":
    model = DQN().to(device)    # 네트워크 모델 정의 후 device 할당
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)    # 최적화기 정의(Adam optimizer)

    agent = DQNAgent(model, optimizer)  # DQNAgeng class를 agent로 설정

    model.train()   # 딥러닝 네트워크를 학습 모드로 설정

    # 스텝, 에피소드, reward_list, loss_list, max_Q_list 초기화
    step = 0
    episode = 0
    reward_list = []
    loss_list = []
    max_Q_list = []

    # 게임 진행을 위한 반복문
    while step < run_step + test_step:
        # 상태, episode, reward, done 정보 초기화
        state = env.reset()
        episode_rewards = 0
        done = False

        # 에피소드를 위한 반복문
        while not done:
            # 학습을 run step까지 진행하여 학습이 끝난 경우,
            # 학습 모드를 False로 설정하고 네트워크를 검증 모드로 설정
            if step == run_step:
                train_mode = False
                model.eval()
            
            # 행동 결정
            action = agent.get_action(state)

            # 다음 상태, 보상, 게임 종료 여부 정보 취득
            next_state, reward, doen, _ = env.step(action)

            # episode_rewards에 보상을 더해줌
            episode_rewards += reward

            # 검증 모드인 경우, epsilon을 0으로 설정
            if train_mode == False:
                agent.epsilon = 0.0
            
            # 상태 및 스텝 정보 업데이트
            state = next_state
            step += 1

            if train_mode:
                # Epsilon 감소
                if agent.epsilon > epsilon_min:
                    agent.epsilon -= 1 / run_step
                
                # 모델 학습
                loss, maxQ = agent.train_model(state, action, reward, next_state, done)
                loss_list.append(loss)
                max_Q_list.append(maxQ)
            
                # 모델 저장
                if step % save_step == 0 and step != 0 and train_mode:
                    agent.save_model(load_model, train_mode)
                
            reward_list.append(episode_rewards)
            episode += 1

            # 진행상황 출력
            if episode % print_episode == 0 and episode != 0:
                print("step: {} | episode: {} | reward: {:.2f} | loss: {:.4f} | maxQ: {:.2f} | epsilon: {:.4f}".format(
                    step, episode, np.mean(reward_list), np.mean(loss_list), np.mean(max_Q_list), agent.epsilon))
                
                reward_list = []
                loss_list = []
                max_Q_list = []
        
        # 학습 및 검증 종료 이후 모델 저장
        agent.save_model(load_model, train_mode)
        env.close()
```

### Frame Skipping & Stacking
- Frame Stacking
    - 공은 어느 방향으로, 어느 속도로 날아가고 있을까?
    - 연속적인 프레임을 시간 순서대로 보면, 공의 방향과 속도를 알 수 있다.
    - 시간에 대해 연속적인 프레임을 모두 한번에 CNN의 input으로 이용한다.
    - Image를 시간에 따라 stack하여 CNN의 input으로 이용
- Frame Skipping
    - 일반적으로 게임은 짧은 시간 안에 여러 프레임이 진행
        - ex. 1초에 30프레임
    - 가까이 있는 프레임들은 큰 차이가 없다. ---> 학습이 잘 안됌
    - 몇 프레임을 건너뛰면서 frame stacking을 수행
        - 시간에 따른 변화가 분명함
- 논문의 방식: 4 frame skip, 3 frame stacking
    - Agent는 매 4번째 프레임들을 stack하여 CNN의 input으로 이용
    - 최신 프레임 이후 skip되는 프레임 동안 같은 action을 반복
    - $s_1 = (x_1, x_2, x_3)$: $x_4$ 가 되기 전까지 $s_1$ 을 통해 선택한 action을 반복
    - $s_2 = (x_2, x_3, x_4)$: $x_5$ 가 되기 전까지 $s_2$ 을 통해 선택한 action을 반복
    - $s_3 = (x_3, x_4, x_5)$: $x_6$ 가 되기 전까지 $s_3$ 을 통해 선택한 action을 반복
- 본 강의의 방식: 4 frame skip, 3 frame stacking
    - ex. 테트리스나 뱀 게임 같은 형태의 게임
    - 같은 action이 반복되면 안되는 게임
    - 매 step action을 선택해야 한다.
    - Agent는 매 4번째 프레임들을 stack하여 CNN의 input으로 이용
    - 매 프레임 마다 frame skipping과 stacking을 수행
    - $s_1 = (x_1, x_2, x_3), s_2 = (x_2, x_3, x_4), s_3 = (x_3, x_4, x_5)$


### Frame Skipping & Stacking 코드
- Parameters
    - state_set: 지난 state들의 history들을 저장해 놓은 list(num_skipping * num_stacking 만큼 저장)
        - state_set = [$s_5, s_9, s_{10}, \dots, s_{19}, s_{20}]: num_skipping = 4, num_stacking = 4인 경우 총 16개 저장
        - $s_{20}$: 현재 state
    - num_skipping: skip 할 frame의 수
        - num_skipping = 4: $\textcolor{red}{s_{20}}, s_{19}, s_{18}, s_{17}, \textcolor{red}{s_{16}}, s_{15}, s_{14}, s_{13}, \textcolor{red}{s_{12}}, s_{11}, s_{10}, s_{9}, \textcolor{red}{s_{8}}, s_{7}, s_{6}, s_{5}$
    - num_stacking: stack할 frame의 수
        - num_stacking = 4: [$s_{20}, s_{16}, s_{12}, s_{8}$]

```python
# parameters for skipping and stacking
self.state_set = []
self.num_skipping = 4
self.num_stacking = 4
```

- Initialization

```python
# Define game state
game_state = game.GameState()

# Initialization
state = self.initialization(game_state)             # state 초기화 및 state_set 설정
stacked_state = self.skip_and_stack_frame(state)    # frame_skipping & stacking 수행


def initialization(self, game_state):
    action = np.zeros([self.num_action])            # 아무 행동도 취하지 않는 action
    state, _, _ = game_state.frame_step(action)     # frame_step에 action을 대입하여 state를 얻음
    state = self.reshape_input(state)               # state의 크기를 학습에 이용할 크기대로 변경(80x80)

    # num_skipping * num_stacking의 수 만큼 리스트에 state를 추가하여 state_set 초기화
    for i in range(self.num_skipping * self.num_stacking):
        self.state_set.append(state)
    
    return state    # 초기화한 state를 반환: skip_and_stack_frame 함수의 inputㅇ로 이용

```


- Skip and stack frame

```python

def skip_and_stack_frame(self, state):
    self.state_set.append(state)    # 새로운 state를 state_set에 추가

    # state_in은 (이미지 사이즈 * 이미지 사이즈 * stacking할 사이즈)로 초기화
    state_in = np.zeros((self.img_size, self.img_size, self.num_stacking))

    # Stack the frame according to the number of skipping frame
    # state_set 내부의 state들을 이용해 skipping & stacking 수행
    #   - s_8 : state_in[:, :, 3] = self.state_set[-1-(4*3)]
    #   - s_12: state_in[:, :, 2] = self.state_set[-1-(4*2)]
    #   - s_16: state_in[:, :, 1] = self.state_set[-1-(4*1)]
    #   - s_20: state_in[:, :, 0] = self.state_set[-1-(4*0)]
    for stack_frame in range(self.num_stacking):
        state_in[:, :, stack_frame] = self.state_set[-1 - (self.num_skipping * stack_frame)]

    # state_set의 가장 이전 데이터 삭제
    del self.state_set[0]

    # state_in을 uint8로 변환
    # 메모리를 위해 크기를 줄인다. (Experience replay에 이용)
    state_in = np.uint8(state_in)

    # 최종 state_in을 반환
    return state_in
```

- Main 함수

```python
# while 문 내부에서도 next_state를 받은 다음에 reshape하고, skip_and_stack_frame 함수의 input으로 이용
# stack 된 input을 학습에 이용한다.
while True:
    # Get progress:
    self.progress = self.get_progress()

    # Select action
    action = self.select_action(stacked_state)

    # Tack actino and get info. for update
    next_state, reward, terminal = game_state.frame_step(action)
    next_state = self.reshape_input(next_state)
    stacked_next_state = self.skip_and_stack_frame(next_state)

    # update forer info.
    # 1 step이 끝난 뒤에도 state = next_state가 아닌 stack된 state를 이어받는다.
    stacked_state = stacked_next_state
    self.score += reward
    self.step += 1

    # If game is over(terminal)
    # Terminal의 경우에도 state가 아닌 stacked_state로 초기화
    if terminal:
        stacked_state = self.if_terminal(game_state)
```


### Experience Replay
- 일반적으로 게임은 짧은 시간 안에 여러 프레임이 진행
    - ex. 1초에 30프레임
- 가까이 있는 프레임들은 큰 차이가 없다. ---> 학습이 잘 안됨
- 멀리 있는 프레임들은 서로 많이 다르다. ---> 학습이 잘 됨
- 진행 과정
    1. 매 step마다 experience를 저장
        - Experience $e_t = (s_t, a_t, r_t, t_t, s_{t+1})$
        - Dataset $D_t = {e_1, e_2, \cdots, e_t}$ : Replay Memory
    2. 임의로 sampling: 연속적인 sample들 간의 연결관계를 깨준다.
        - $e_2 = (s_2, a_2, r_r, t_2, s_3)$
        - $e_6 = (s_6, a_6, r_6, t_6, s_7)$
        - ...
        - $e_9 = (s_9, a_9, r_9, t_9, s_{10})$
    3. Mini Batch Update


### Target Network
- $\theta$: 뉴럴 네트워크의 변수들(weights, bias)
    - 매 step마다 update
    - 다음 항목들을 계산
        - $a_t = \arg\max_a Q(s_t, a; \theta)$
        - Loss: $y - Q(s_t, a; \theta)$
- $\theta^-$: Target Network
    - 매 C step마다 한번씩 네트워크를 복제
    - target value를 계산하는데 이용
        - $
        y = 
            \begin{cases}
                & r \newline
                & r + \gamma(\max_{a'} Q(s_{t+1}, a'; \theta^-))
            \end{cases}
        $
- 매 C step마다 복사
- Target이 흔들리지 않도록 함
- 기존 DQN에 비해 훨씬 안정적인 학습

```python
import numpy as np
import random
import datetime, os     # 학습된 모델을 저장하기 위해 사용
from collections import deque       # 리스트와 유사하게 데이터를 저장
                                    # 최대 길이를 넘어서 데이터가 저장되는 경우,
                                    # 가장 오래된 데이터부터 자동으로 삭제
import gym

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn

# Environment
env = gym.make("CartPole-v0")

# Parameter Setting
algorithm = "DQN"

state_size = 4                      # 상태의 수 정의
action_size = env.action_space.n     # 행동의 수 정의

load_model = False      # 학습된 모델을 불러올지 결정
train_mode = True       # 학습을 수행할지 결정

batch_size = 32         # 한 스텝당 batch size의 수 만큼 데이터를 이용하여 학습
mem_maxlen = 10000      # Replay Memory의 최대 길이

skip_frame = 1          # 몇 개의 frame을 skip할지 결정
stack_frame = 1         # 몇 개의 frame을 stack할지 결정

start_train_step = 10000    # Replay memory에 일정 개수 이상 데이터를 채우고 학습 수행
run_step = 50000            # 학습을 진행할 스텝
test_step = 10000           # 학습 후 테스트를 진행할 스텝

target_update_step = 1000   # target network를 업데이트하는 스텝
print_episode = 10      # 해당 에피소드마다 한번씩 진행 상황 출력
save_step = 20000       # 해당 스텝마다 네트워크 모델 저장

epsilon_init = 1.0      # 초기 epsilon
epsilon_min = 0.1       # epsilon의 최소값

discount_factor = 0.99
learning_rate = 0.00025

date_time = datetime.datetime.now().strftime("%Y%m%d-%H-%M-%S")

save_path = "./saved_models/" + date_time
load_path = "./saved_models/20210224-23-14-51DQN"

# 딥러닝 연산을 위한 device 결정
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# 딥러닝 모델
class DQN(nn.Module):
    def __init__(self, network_name):   # 일반 네트워크와 타겟 네트워크 2가지 모델 이용
        super(DQN, self).__init__()
        input_Size = state_size * stack_frame

        # 네트워크 모델: 3층의 은닉층으로 구성된 인공신경망
        self.fc1 = nn.Linear(input_Size, 512)
        self.fc2 = nn.Linear(512, 512)
        self.fc3 = nn.Linear(512, action_size)
    

    def forward(self, x):
        # 입력: 상태
        # 출력: 각 행동에 대한 Q 값
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x



# DQNAgent 클래스: DQN 알고리즘을 위한 다양한 함수 정의
class DQNAgent():
    def __init__(self, model, target_model, optimizer):
        # 클래스의 함수들을 위한 값 설정
        self.model = model                  # 네트워크 모델 정의
        self.target_model = target_model    # 타겟 모델 정의
        self.optimizer = optimizer          # 최적화기 정의
        
        self.memory = deque(maxlen=mem_maxlen)              # Replay memory 정의
        self.obs_set = deque(maxlen=skip_frame*stack_frame) # 상태를 stack하기 위핸 obs_set

        self.epsilon = epsilon_init

        self.update_target()    # 타겟 네트워크 업데이트

        if load_model == True:
            self.model.load_state_dict(torch.load(load_path+"/model.pth"))
            print("Model is loaded from {}".format(load_path+"/model.pth"))
    

    # Epsilon greedy 기법에 따라 행동 결정
    def get_action(self, state):
        if train_mode:
            if self.epsilon > np.random.rand():
                # 랜덤하게 행동 결정
                return np.random.randint(0, action_size)
        
        with torch.no_grad():
            # 네트워크 연산에 따라 행동 결정
            Q = self.model(torch.FloatTensor(state).unsqueeze(0).to(device))
            return np.argmax(Q.cpu().detach().numpy())


    # 상태에 대해 frame skipping과 frame stacking을 수행
    def skip_stack_frame(self, obs):
        self.obs_set.append(obs)    # obs_set에 상태 추가

        state = np.zeros([state_size * stack_frame])    # 상태를 stack할 빈 array 생성

        # skip frame마다 한번씩 obs를 stacking
        # obs_set 내부에서 frame skipping을 수행하면 설정값 만큼 frame stacking 수행
        for i in range(stack_frame):
            state[state_size*i:state_size*(i+1)] = self.obs_set[-1-(skip_frame * i)]
        
        # 상태를 stack하여 최종 상태로 반환
        return state


    # Replay memory에 데이터 추가(상태, 행동 보상, 다음 상태, 게임 종료 여부)
    def append_sample(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))


    # 네트워크 모델 저장
    def save_model(self, load_model, train_mode):
        # 처음 모델을 학습하는 경우, 폴더 생성 및 save_path에 모델 저장
        if not load_model and train_mode:   # first training
            os.makedirs(save_path + algorithm, exist_ok=True)
            torch.save(self.model.state_dict(), save_path + algorithm + "/model.pth")
            print("Save Model: {}".format(save_path + algorithm))
        # 기존에 학습된 모델을 불러온 뒤 이어서 학습하는 경우, load_path에 모델 저장
        elif load_model and train_mode:     # additional training
            torch.save(self.model.state_dict(), load_path + "/model.pth")
            print("Save Model: {}".format(load_path))

    
    # 학습 수행을 진행하는 train_model 함수
    # 하나의 데이터가 아닌 batch 데이터를 통해 학습을 수행
    def train_model(self):
        # 학습을 위한 미니 배치 데이터 샘플링
        batch = random.sample(self.memory, min(len(self.memory), batch_size))  # memory에서 batch_size의 수 만큼 데이터 취득

        # batch에서 각각의 정보를 얻은 후 이를 array로 쌓아둠
        # Torch의 tensor로 변환하여 device에 올려줌
        state_batch         = torch.FloatTensor(np.stack([b[0] for b in batch], axis=0)).to(device)
        action_batch        = torch.FloatTensor(np.stack([b[1] for b in batch], axis=0)).to(device)
        reward_batch        = torch.FloatTensor(np.stack([b[2] for b in batch], axis=0)).to(device)
        next_state_batch    = torch.FloatTensor(np.stack([b[3] for b in batch], axis=0)).to(device)
        done_batch          = torch.FloatTensor(np.stack([b[4] for b in batch], axis=0)).to(device)

        # 실제 에이전트가 취한 행동에 대한 Q값 도출
        # Action = [1, 2, 0] ---> One hot action = [0, 1, 0], [0, 0, 1], [1, 0, 0]
        # Q 값                   One hot action
        # [[0.1  0.4  0.7],     [[0  1  0],         [[ 0  0.4  0],                 [[0.4],
        #  [0.3  0.2  0.4],  *   [0  0  1],     =    [ 0   0  0.4],  -- sum(1) -->  [0.4],
        #  [0.8  0.1  0.3]]      [1  0  0]]          [0.8  0   0]]                  [0.8]]
        eye = torch.eye(action_size).to(device)
        one_hot_action = eye[action_batch.view(-1).long()]
        q = (self.model(state_batch) * one_hot_action).sum(1)

        # 타겟의 식에 따라 타겟값 계산
        with torch.no_grad():
            max_Q = torch.max(q).item()
            next_q = self.target_model(next_state_batch)    # 다음 상태의 Q값은 타겟 네트워크를 통해 도출
            target_q = reward_batch + next_q.max(1).values*(discount_factor*(1 - done_batch))   # 모든 batch 데이터에 대한 타겟값 계산
        
        # 예측값(q)와 타겟값(target_q) 사이의 손실 함수값 계산(smooth L1 Loss)
        loss = F.smooth_l1_loss(q, target_q)

        # 인공신경망 학습
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # loss와 Q값 반환
        return loss.item(), max_Q


    # 타겟 네트워크 업데이트
    def update_target(self):
        # 일반 네트워크의 모든 변수들을 타겟 네트워크에 복제
        self.target_model.load_state_dict(self.model.state_dict())
    

# 메인 함수
if __name__ == "__main__":
    model = DQN("main").to(device)          # 네트워크 모델 정의 후 device 할당
    target_model = DQN("target").to(device)  # 타겟 네트워크 정의
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)    # 최적화기 정의(Adam optimizer)

    agent = DQNAgent(model, target_model, optimizer)  # DQNAgeng class를 agent로 설정

    model.train()   # 딥러닝 네트워크를 학습 모드로 설정

    # 스텝, 에피소드, reward_list, loss_list, max_Q_list 초기화
    step = 0
    episode = 0
    reward_list = []
    loss_list = []
    max_Q_list = []

    # 게임 진행을 위한 반복문
    while step < run_step + test_step:
        # 상태, episode, reward, done 정보 초기화
        obs = env.reset()   # 단일 상태를 obs로 저장
        episode_rewards = 0
        done = False

        # 에피소드를 처음 시작하는 경우 obs_set을 동일 obs로 skip_frame * stack_frame 수 만큼 채워줌
        for i in range(skip_frame * stack_frame):
            agent.obs_set.append(obs)
        
        state = agent.skip_stack_frame(obs) # frame_skipping & stacking 수행 후 최종 상태로 사용

        # 에피소드를 위한 반복문
        while not done:
            # 학습을 run step까지 진행하여 학습이 끝난 경우,
            # 학습 모드를 False로 설정하고 네트워크를 검증 모드로 설정
            if step == run_step:
                train_mode = False
                model.eval()
            
            # 행동 결정
            action = agent.get_action(state)

            # 다음 상태, 보상, 게임 종료 여부 정보 취득
            next_obs, reward, done, _ = env.step(action)

            # episode_rewards에 보상을 더해줌
            episode_rewards += reward

            # 다음 상태도 frame skipping & stacking 수행 후 next state로 사용
            next_state = agent.skip_stack_frame(next_obs)

            # 학습의 안정화를 위채 추가한 코드
            # - 카트가 최대한 중앙에서 벗어나지 않도록 학습
            # - 카트가 중앙에서 벗어날수록 패널티 부여
            # - 폴이 쓰러지지 않더라도, 카트가 중앙에서 너무 멀어지면서 학습이 제대로 수행되지 않는 경우 발생
            reward -= abs(next_obs[0])

            # 학습 모드인 경우, Replay memory에 경험 데이터 저장
            if train_mode:
                agent.append_sample(state, action, reward, next_state, done)
            # 검증 모드인 경우, epsilon을 0으로 설정
            else:
                agent.epsilon = 0.0
                # 해당 라인의 주석을 해제하면 학습이 끝나고 검증 수행시 게임 화면을 확인할 수 있음
                env.render()
                
            if train_mode:
                # Epsilon 감소
                if agent.epsilon > epsilon_min:
                    agent.epsilon -= 1 / run_step
                
                # 모델 학습
                loss, maxQ = agent.train_model()
                loss_list.append(loss)
                max_Q_list.append(maxQ)
            
                # 모델 저장
                if step % save_step == 0 and step != 0 and train_mode:
                    agent.save_model(load_model, train_mode)

                # 타겟 네트워크 업데이트
                # 일정 스텝마다 타겟 네트워크 업데이트 수행
                if step % target_update_step == 0:
                    agent.update_target()


            # 상태 및 스텝 정보 업데이트
            state = next_state
            step += 1
                
        reward_list.append(episode_rewards)
        episode += 1

        # 진행상황 출력
        if episode % print_episode == 0 and episode != 0:
            print("step: {} | episode: {} | reward: {:.2f} | loss: {:.4f} | maxQ: {:.2f} | epsilon: {:.4f}".format(
                step, episode, np.mean(reward_list), np.mean(loss_list), np.mean(max_Q_list), agent.epsilon))
            
            reward_list = []
            loss_list = []
            max_Q_list = []
        
    # 학습 및 검증 종료 이후 모델 저장
    agent.save_model(load_model, train_mode)
    env.close()
```