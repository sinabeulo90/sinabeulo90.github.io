---
layout: post
title:  "Chapter 9.3 확률론적 경사도와 준경사도 방법"
date:   2021-01-17 00:00:00 +0900
categories: "Reinforcement Learning"
plugins: mathjax
---

## 9.3 확률론적 경사도와 준경사도 방법

## 9.3.1 확률론적 경사도 강하<sup>Stochastic-Gradient Descent, SGD</sup> 방법

확률론적 경사도 강하 방법은 가치 예측 과정에서 함수 근사를 하기 위한 한 종류의 학습 방법으로 모든 함수 근사 방법 중 가장 폭 넓게 사용되는 방법이다. 특히 온라인 강화학습 방법에 잘 들어 맞는다.

가중치 벡터 $\mathbf{w}$를 고정된 개수의 실수값을 원소로 갖는 열 벡터 $(w_1, w_2, \cdots, w_d)^T$라 하고, 근사적 가치 함수 $\hat{v}(s, \mathbf{w})$를 모든 $s \in S$에 대해 미분 가능한 $\mathbf{w}$ 함수라고 하면, 이산적 시간 단계 $t = 0, 1, 2, \cdots$ 동안 각 시간 단계에서 갱신을 수행한다. 이때 각 시간 단계에서의 가중치 벡터는 $\mathbf{w}_t$로 표현할 수 있다.

문제를 좀더 단순히 하기 위해, 무작위로 선택된 상태 $S_t$와 해당 정책 하에서 상태 $S_t$와 $S_t$의 실제 가치로 구성되는 새로운 예제 , 그 상태의 실제 가치로 구성되는 예제 $S_t \mapsto v_\pi(S_t)$를 각 시간 단계에서 관측한다고 가정하자. 사실 이렇게 구성되는 예제들은 환경과 상호작용을 통해 발생하는 연속적인 상태들 일테지만, 일단 지금을 그렇게 가정하지 않는다. 왜냐하면 1. 각 상태 $S_t$에 대한 정확한 가치 $v_\pi(S_t)$를 알고 있더라도, 함수 근사는 제하뇐 자원으로 부터 비롯된 제한된 해상도로 수행된다는 점, 2. 모든 상태 또는 심지어 모든 예제에 정확하게 맞는 가중치 벡터 $\mathbf{w}$는 일반적으로 존재하지 않는다는 점, 3. 예제에 포함되지 않은 다른 모든 상태에 대해서도 일반화가 필요하다는 점 등 여전히 어려운 문제들이 남아있기 떄문이다. 그리고 예제에 포함된 상태는 동일한 분포 $\mu$를 갖는다고 가정한다.

확률론적 경사도 강하<sup>SGD</sup> 방법은 관측된 예제에 대한 오차($\overline{VE}$)를 최소화하는 것으로, 각 예제에 대한 오차가 가장 많이 감소되는 방향으로 가중치 벡터 $\mathbf{w}$를 조금씩 조정하여 관측된 예제를 최소화한다.

$$
\begin{align*}
\mathbf{w}_{t+1} &\doteq \mathbf{w}_t - \frac{1}{2} \alpha \bigtriangledown[v_pi(S_t) - \hat{v}(S_t, \mathbf{w})]^2 \newline
&= \mathbf{w}_t + \alpha [v_pi(S_t) - \hat{v}(S_t, \mathbf{w})] \bigtriangledown \hat{v}(S_t, \mathbf{w}_t)
\end{align*}
$$

