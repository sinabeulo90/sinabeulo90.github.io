---
layout: post
title:  "Exploration and Exploitation"
date:   2021-03-18 01:00:00 +0900
category: "Reinforcement Learning"
tags: YouTube
---

## 참고

- video
    - [[강화학습 9강] Exploration and Exploitation](https://youtu.be/nm6RwuA_pGE)
- slide
	- [Lecture 9: Exploration and Exploitation](https://www.davidsilver.uk/wp-content/uploads/2020/03/XX.pdf)


## Exploration vs. Exploitation Dilemma
- 우리가 실시간으로 의사결정을 할 때, 굉장히 근본적인 문제가 있다.
    1. Exploitation: 현재 내가 알고있는 정보를 바탕으로 제일 좋을 것 같은 선택을 한다. 현재 알고 있는 정보를 바탕으로 하기 때문에, 정보가 모여야 하는데, 이를 exploration이 수행한다.
    2. Exploration
- 내가 계속 결정을 하는데, 어쩔 때는 정보를 더 모으기 위해서 제일 좋은 선택지가 아님에도 불구하고 실험적인 수들을 선택할 수 있고, 이 수들을 바탕으로 지금까지 모인 정보를 바탕으로 최적의 결정을 내릴 수 있다.
- 그런데 이 둘은 굉장히 trade-off인 관계이다. 본질적으로 이 둘은 동시에 만족할 수 없기 때문이다.
- 우리가 결국에는 잘하는 agent를 만들고 싶은데, 이를 위해서 이 두개를 꾸준히 수행해주어야 한다. exploitation만 한다면 현재 내가 알고 있는 것만 바탕으로는 최적의 결정을 선택할 수는 있겠지만, 알고있는 것을 늘리기 위해서는 exploration을 해주어야 한다.
- agent는 이 2개 사이에서 줄타기를 계속 해주어야 한다.



## Example
- exploitation과 exploration은 강화학습에서 근본적인 문제이다. 우리 삶에서도 자주 맞닥드리는 문제이다.



## Principles
- 이 둘 사이의 균형을 어떻게 맞출까?
    1. e-greedy: epsilon이라는 작은 확률로 랜덤하게 다르 행동을 선택한다.
        - greedy: 내가 알고 있는 것을 바탕으로 최적의 선택을 한다.
        - 굉장히 naive한 방법론이고, 단순하지만 굉장히 강력하다.
        - greedy policy에 noisy(random성)를 추가하는 것 또는 softmax를 사용하여 확률분포로 선택
    2. Optimism in the Face of Uncertainty
        - systematic한, 좀더 체계적인 접근법
        - 불확실성을 긍정적으로 보는 방법론
    3. Information State Search
        - 가장 정확한 방법이지만, 계산적으로는 어려운 방법론
        - 내가 어느 정도를 알고 있음을 state에 포함시켜서 접근하는 방법론



## The Multi-Armed Bandit
- Multi-Armed Bandit을 하는 이유는 MDP보다 훨씬 간단한 문제이기 때문이다. 이 간단한 문제에서 아이디어를 적용하고 나면, 나머지는 이 아이디어를 확장해서 적용하면 되기 때문에, multi-armed bandit에서 이야기가 주로 이루어진다.
    - MDP를 정의하기 위해서는 state space s와 transition probability P와 a, r, \gamma가 필요한데, multi-armed bandit은 오직 a와 r만 필요하다.
    - 예를 들어, 카지노의 bandit machine을 할 때, 각 기계들마다 reward의 확률분포와 기댓값이 다르다고 하자. 어떤 machine은 평균이 100만원인데 variance가 클 수가 있고 (0 ~ 1000만원), 어떤 machine은 꾸준히 10만원이 나오는 machine일 수가 있다. 즉 각 bandit machine 마다 자신만의 확률분포가 있어서 거기서 샘플링된 값이 reward로 주어진다.
        - 이 문제가 간단한 이유는 사실 state가 없다. 왜냐하면 단지 machine을 하나 정해서 당기면 게임이 끝나기 때문이다. 따라서 multi-armed bandit을 one-step MDP라고 부르기도 한다. action과 reward set만 있으면 된다. 내가 할 수 있는 action이 있고, 그 action을 했을 때 reward 분포가 있으면 multi-armed bandit 문제가 정의된다.
- m개의 bandit machine이 있고, 당겼을 때 reward의 분포는 알 수 없다. 우리는 당기면서 이 분포를 알아나가야 한다.
- 각 step 마다 하나의 machine을 당긴다. 그러면 환경이 reward를 만들어 주고, cumulative reward를 maximise한다. 한번 당겼을 때, maximise 하는 것이 아니라, 1000만원이 예산이 있을 때, 전체 보상을 최대하고자 한다.(MAB)
- 딱 봐도, exploration과 exploitation이 trade off이다.
    - 내가 machine을 몇개 당겨봤는데, 잘 나오면 걔네만 당길 수도 있지만, 안당겨본 bandit machine이 엄청 좋은 bandit machine 일 수도 있어서, 가끔은 그 bandit machine을 좀 당겨줘야 할 것이다.
    - trade off를 다루기 좋은 문제이다.



## Regret
- 후회를 어떻게 정의할까
- action-value: 어떤 action을 했을 때 bandit machine reward의 기댓값
- optimal value V^*: 내가 당길 수 있는 각각의 machine의 기댓값 중에서 제일 기댓값이 높은 것
    - 모든 action들 중에 제일 큰 Q값
- regret: 내가 어떤 action을 했을 때, 해당 bandit machine의 기댓값과 optimal value V^*의 차이
    - 하지만, optimal value V^* 모르기 떄문에, regret을 정확히 알 수는 없다.
    - action별로 정의되는 개념
- total regret: 예를 들어 100번 당기면, 100번동안의 regret을 모두 더한 것
- 즉 cumulative reward를 최대화하는 것은 total regret을 최소화하는 것과 같다.
    - 그냥 reward를 maximize하면 되지, 왜 regret이라는 개념을 도입해서 접근을 할까?
        - 어떤 알고리즘이 가장 잘 할 수 있는 있는지를 이해하는데 도움을 주기 때문이다.



## Counting Regret
- count N_t(a): action a를 수해한 횟수
- gap: 제일 좋은 bandit machine에서 해당 bandit machine의 value를 뺀 것
    - 각 bandit machine 별로 각 gap이 존재
    - gap은 action에 의해 정의되는대, 해당 machine이 제일 좋은 machine과 비교해서 얼마나 안좋은 지를 나타낸다.
- regret는 gap과 count의 함수가 된다.
    - 모든 action에 대해서, action a를 당긴 횟수에 해당 a의 gap을 곱한 것의 합이 total regret이다.
- 좋은 알고리즘은 gap이 큰 machine에서 작은 count를 보장한다.
- 하지만 문제는 우리는 gap을 알지 못한다.



## Linear or Sublinear Regret
- x축: machine을 당긴 횟수
- y축: total regret
    - gap들의 차이의 기댓값
- 어떤 알고리즘이 5% 확률로 평생 exploration 한다고 가정하자. 그러면 linear한 total regret line이 그려진다.
    - 왜냐하면 항상 5%의 확률로 바보같은 행동을 할 수밖에 없기 때문에 그 값이 계속 더해진다.
- 어떤 알고리즘이 평생 exploration을 하지 않을 경우(greedy), 이 때도 linear한 선형 곡선이 그려진다.
    - 왜냐하면 운 좋게 제일 좋은 action을 greedy하게 선택하지 않는 한, 제일 좋은 action과 내가 선택한 action의 gap이 있기 때문에 그 gap 만큼 linear하게 계속 누적된다.
    - greedy 알고리즘이 sub optimal action을 계속 당기게 되면 거기에 갖혀서, regret이 쌓이므로, linear한 total regret이 나온다.
- 그렇다면 우리가 어떤 알고리즘을 만들어서 적절하게 exploration과 exploitation을 하면 sublinear한 total regret 그래프를 그릴 수 있지 않을까?
    - 그릴 수 있다.



## \epsilon-Greedy Algorithm
- gap들의 합이 있고, 그것을 epsilon의 확률로 계속 선택하므로 계속 regret이 쌓인다.



## Optimistic Initialisation
- 모든 value들을 maximum으로 낙관적으로 초기화한다. 예를들어, machine을 당기는데, 모든 machine이 1억원의 기댓값을 가지고 있다고 초기화를 하고, greedy하게 선택한다.
    - 간단한 아이디어
    - Q를 high value로 초기화하고, 그 다음부터 Monte-Carlo 방법을 수행한다. 여기서는 평균을 효율적으로 사용했다.
- 모두 1억원의 기댓값을 가지고 있다면, 처음에는 random하게 machine을 선택할 것이다. 그리고 하나를 당겼을 때 100만원이 나오면, 1억과 100만원의 평균을 해당 mahine의 value로 삼는다. 그러면 다른 machine을 당길 것이다. 이 mahine에서 0이 나왔다면, 마찬가지로 1억과 0의 평균을 해당 machine의 value로 삼는다. 이렇게 처음에는 모두 높은 value를 가지다가, 당길 때마다 value가 줄어들게 된다. 그렇게 되면 처음에 모두 높기 때문에, 당기지 않은 것들 부터 당기게 될 것이다. 초반에는 그렇게 하겠지만, 어느 정도 자리를 잡고 나면서 높은 value를 가지는 machine만 선택한다.
    - 매 회 action을 선택할 수록, 처음에 높은 value가 해당 machine의 실제 value scale에 내려올때까지 충분히 exploration이 된다.
- exploration과 exploitation이 trade off하면서 줄타기를 하고, 특히 초기 단계에서 exploration을 권장하게 된다.
- 하지만, 이 방법이 제일 좋은 actio으로 수렴한다는 보장이 없다. 운이 나쁘면, 3등정도 하는 machine이 좋은게 나와서 거기에 갖힐 수도 있다.
    - greedy + optimistic initialisation: linear total regret
    - e-greedy + optimistic initialisation: linear total regret
    - 이 방법은 어떤 하나의 초기화 하는 방법론이기 때문에, greedy든 e-greedy든 사용할 수 있다. 이 2가지(greedy, e-greedy)를 사용해도 linear total regret을 벗어나지 못하지만, 이건 충분히 좋은 아이디어이고, 실제로 실무에서 좋은 아이디어라는 인상을 가졌으면 한다.



## Decaying \epsilon_t-Greedy Algorithm
- 어떤 스케쥴을 갖고 \epsilon을 계속 줄여나간다.
    - 직관적으로는, 처음에는 좀 높은 \epsilon을 갖고 exploration을 점점 많이 해보다가, 점점 정보가 많이 쌓이면 어느 하나에 수렴해야하기 때문에 \epsilon을 줄여나간다.
    - 가능한 여러가지 스케쥴이 있겠지만, 여기서는 하나의 스케쥴을 예시로 heuristic을 든다.
        - d: 1등 machine과 2등 machine의 regret 차이, 2등 machine이 1등 machine에 얼마나 딸리는가
        - d가 작으면 분모가 작아지므로 epsilon이 커지고, 반대로 d가 커지면 epsilon이 충분히 작아진다. 즉 1등과 2등이 미세하게 차이가 날 경우 exploration을 많이 해보고, 조금 차이가 날 경우 적게 해본다.
        - t가 분모에 있기 때문에 step마다 갈수록 epsilon이 줄어들게 된다.
        - action이 많을 수록 epsilon이 크다.
        - c: 충분히 작은 상수 값
- decay e-greedy는 total regret이 로그함수 형태를 나타낸다!
    - 지금까지는 total regret이 linear 형태였는데, 여기서 처음으로 log 함수 형태가 나온다.
- 하지만, 스케쥴링을 할 때, 분모에 gap에 대한 정보 d를 사용하는데, 실제로는 우리가 이 gap을 모르기 때문에 그대로 쓸 순 없어서 여러 heuristic을 이용해서 epsilon을 줄여나갈 수는 있다.
- 우리의 목적은 R에 대한 정보 없이도 multi-armed bandit에서 sublinear regret을 갖는 알고리즘을 찾는 것이 목적이다.
    - R에 대한 정보 없이도 decay e-greedy를 쓸수도 있는데, 이 때 heuristic 스케쥴을 잘 정의해주어야 한다. 잘 못 정하면 catastrophic하게 굉장히 망한다.



## Lower Bound
- 어떤 알고리즘도 lower bound가 존재하며, 이 lower bound는 log함수이다. 로그보다 더 아래로 갈 수는 없다.
    - 어떤 천재적인 알고리즘일지라도 로그보다는 더 아래로 갈 수 없다.
- 애초에 이 multi-armed bandit 문제가 이미 가지고 있는 lower bound performance가 있다. 이 performance는 제일 좋은 machine과 다른 machine의 유사도에 의존하므로, 이에 따라 문제의 난이도가 결정이 된다.
    - 아무리 잘해도 더 잘할 수 없는 performance가 존재하는데, 이를 lower bound라고 표현한다.
    - 그리고, 어려운 문제는 굉장히 비슷한 확률분포를 보이는 arm들이 있다. 실제 각각의 평균은 다르지만, 분포가 비슷하다. 이 arm들을 계속 시도해야하는데, 그러면 계속 틀리는 양(regret)이 쌓이게 된다.
- 비슷한 정도의 분포를 계산하는데, KL divergence를 이용할 수 있다.
    - KL divergence: 어떤 두 확률분포 사이의 차이를 나타내는 값
        - 차이가 크면 값이 커지고 유사한 확률 분포일 경우 작은 값을 가진다.
- Lai and Robbins Theorem
    - total regret은 각 action의 gap이 얼마나 큰 지와 그 때의 KL divergence(해당 machine의 확률분포와 1등 machine의 확률분포가 얼마나 비슷한가)로 정의된다.
        - 확률이 비슷할 수록 KL divergence가 작아지고, 이 값은 커지므로 문제가 어려워진다. 즉 알고리즘이 갈 수 있는 lower bound의 한계가 높아지는 것이다. 그리고 gap이 클 수록 문제가 어려워진다.
- 정리하면, 알고리즘을 아무리 잘 짜도 로그함수보다 아래로 내려갈 수는 없을 것이다.
    - 즉, regret이 특정 상수에 수렴하는 것은 이론적으로 불가능하다. 왜냐하면 log는 step에 대해 단조증가 함수로 계속 커지기 때문이다.



## Optimism in the Face of Uncertainty
- 3개의 machine이 있고, 각 machine의 기댓값을 추정해서, 위와 같은 그림과 같은 신뢰구간으로 표현했다고 하자.
- 그럼 우리는 초로색이 신뢰구간이 높은 곳에 있기 때문에, a_3을 당겨보고 싶을 것이다.
- Optimism in the Face of Uncertainty는 uncertainty를 긍정적으로 바라보자는 철학이 있다.
    - 즉 파란색의 95% 신뢰구간을 보면, 파란색의 오른쪽 끝이 매우 높음을 알 수 있다. 현재는 파란색 값에 대해서는 값을 잘 모르는, uncertainty가 굉장히 큰 상태인데, 이렇게 불확실성이 높을 수록, 이 값에 대해 잘 모르기 때문에 더 explore해야 하는 action이 된다. 그래서 지금은 아니지만, 그만큼 이 파란색이 best action으로 바뀔 수도 있다.



## Optimism in the Face of Uncertainty(2)
- 실제로 파란색 action을 선택하면, 파란색 action이 안좋은 선택이었기 때문에 왼쪽으로 더 이동했지만, 동시에 그만큼 불확실성도 낮아지면서 파란색이 주는 value에 대해서 신뢰구간도 줄어들었다.
    - 이제는 제일 좋은 action을 뽑기위해, 다른 action을 뽑을 가능성이 높아졌다.
- 이렇게 uncertainty를 긍정적으로 보자는 철학이 있다.



## Upper Confidence Bounds
- 신뢰구간(confidence interval)이 있을 때, 각 action value에 대해서 upper confidence가 있을 것이다. a를 했을 때 실제 value Q(a)가 추정치 \hat{Q}와 \hat{U}의 합보다는 작을 확률이 높을 것이다.
    - ex. 95%의 신뢰구간이라는 뜻은 실제 Q(a)가 내가 생각하는 추정치 \hat{Q}와 \hat{U}의 합보다는 작을 확률이 95%라는 의미가 된다. (우리가 생각하는 구간이 실제값을 포함할 확률이 95%라는 의미)
- confidence bound 중에 위에 값을 찾을 수 있다.
- 여기서 \hat{Q}는 샘플이 쌓일 수록 점점 정확해 질 것이고, \hat{U}는 N_t{a}가 작아질 수록 \hat{U}가 커지고 신뢰구간이 넓어진다. 신뢰구간이 넓어진다는 것은 같은 정확도를 위해서 더 넓은 구간을 고려해야 하므로, 그만큼 자신이 없다는 것이다. 반면, N_t(a)가 커질 수록 \hat{U}가 작아진다.
- 따라서 우리는 \hat{Q} + \hat{U}를 최대화가 되는 a를 선택할 수 있다.
- 구체화 하는 방법
    - Frequentist view
    - Bayesian approach


## Hoeffiding's Inequality
- 모분포의 distribution을 아예 몰라도 할 수 있는 어떤 Frequentist view가 있는데, 이를 위해 Hoeffiding's Inequality이 필요하다.
- USB의 개념과 함께 섞어서 쓸 수 있는 부등식은 정말 많은데, 그중의 하나이다.
    - ex. Berstein inequality, azuma inequality 등
- 분포와 관계없이 항상 쓸수 있다.
    - X_1 ~ X_t가 0과 1 사이의 random variables라 하고, 이것들의 sample mean이 \bar{X}라고 하자.
    - \mathbb{E}[X]: X의 실제 모평균, 실제 X의 기댓값
    - u: 틀리는 정도
    - t: 샘플링 한 갯수
    - 실제값이 샘플 평균보다 u보다 더 틀릴 확률이 e^{-2tu^2} 이하 이다.
        - 이미 증명이 되어 있는 사실이다.
- 직관적으로 보면, u는 틀린 정도이므로, u가 커질수록 확률은 작아질 것이다. 예를 들어 10만큼 틀릴 확률이 1만큼 틀릴확률보다 낮으므로, 우변 항에 -u^2이 있다. 그리고 t가 많아질 수록, 100개의 평균이 10개의 평균보다 좀더 정확하므로 우변 항에 -t가 붙어있다.
- 이것을 bandit에 적용해보면, action a를 선택할 때, Q(a)가 기댓값이고, \hat{Q_t(a)}가 샘플 평균이고, U_t(a)가 upper confidence bound라고 하면, 맨 밑에 있는 식과 같이 된다.



## Calculating Upper Confidence Bounds
- Upper confidence bounds를 계산해보면, p는 적당히 정해서 풀면 된다.
    - ex. p = 0.95, 95% 확률로 맞추고 싶다.
- 그러면 각 action 별로 Q의 추정치가 있을 텐데, 거기에 U_t(a)를 더한 값 중 가장 큰 값을 가지는 action을 선택하면 된다.
- p를 고정하기보단, p = t^{-4}이라는 스케쥴을 두어서 진행하면, U_t(a)의 root안 분자에 log에 t만 남고, 4가 빠져나오면서 아래식이 된다.
    - 이 식을 UCB1 알고리즘이라고 한다.



## USB1
- 각 action별 N_t(a)와 Q(a)를 활용해서 위의 식을 maximise하는 action을 선택한다.
- 우리가 결국에는 upper confidence bound 계산하려고 하는데, Frequentist 라서 모분포를 모르고 할 때, Hoeffiding's 부등식에 떄려박아서 upper confidence bound를 계산했다.
- 즉 Q + U가 최대가 되는 action을 고르면, exploration과 exploitation을 잘 하면서 total regret 함수가 log가 된다.



## Example: UCB vs. e-Greedy On 10-armed Bandit
- bandit machine이 10개가 있고, 각 machine의 reward는 같은데 확률이 다르다고 하고, e-greedy와 USB의 performance를 비교하자.
    1. 90% 1개, 60% 9개
    2. 90% 1개, 80% 3개, 70% 3개, 60% 3개
- 놀랍게도, e-greedy가 꽤 잘한다. 대신 e-greedy는 튜닝을 잘못하면 재앙이 된다.
    - 우상단 그래프에서 e-greedy가 0.05인 경우를 보면, regret이 올라가면서 망한 것을 볼 수 있다.
    - e-greedy가 잘 설정하면 USB보다 좋은 경우가 많다.
- 반면 USB는 항상 잘 하고 있다.
- 좌측 그래프의 y축은 제일 좋은 machine을 선택한 비율
    - 결국에는 제일 좋은 machine으로 수렴해야 하는데, 거의 다 제일 좋은 machine에 수렴한다.
- USB가 왜 e-greedy와 비슷한 성능을 내는가?
    - Hoeffiding's 부등식이 굉장히 약한 부등식이다. Hoeffiding's 부등식은 모분포에 대해 가정하는 것이 별로 없다. 심지어 분포가 symmetrical(대칭적)하지 않아도 된다. 다만 여기서 가정하는 것은 reward가 bounded되어 있다고 가정한다. Hoeffiding's 부등식을 보면 0~1 사이의 값을 갖는다는 가정이 있는데, 그 말은 우리의 reward는 0~1 사이의 값이 아니므로 max 값으로 나누어 주면 될텐데, bound가 되어 있어야 0과 1사이의 값으로 나눌 수 있다. 즉 reward가 bounded 되어있다는 가정이 있다면, Hoeffiding's 부등식을 사용할 수 있다. 예를 들면 bandit machine이 아무리 비싸도 1000만원 이상은 안나온다는 가정이 있어야 한다. max값을 아주 크게 설정해도 그 수보다 큰 값이 나올 확률이 있다면 잘못될 것이지만, 그게 아니라면 실무에서 사용할 수 있을 것이다.
    - 다시 말하면, Hoeffiding's 부등식은 가정이 적은 만큼 부등식이 약하다. 다른 부등식들의 경우 가정이 더 쎈 만큼, 조금 더 빡빡한 부등식을 짜서 더 효율이 올라갈 수 있을 것이다(그만큼 다른 정보가 필요할 것이다).



## Bayesian Bandits
- reward distribution에 대한 prior knowledge를 사용
    - prior knowledge는 사람이 가정을 해서 사용한다.(ex. 베르누이 분포, 가우시안 분포)
    - 이 prior knowledge에 bandit을 당겨서 나온 reward를 보고, history를 봐서 posterior 분포를 구한다.
    - 이 posterior를 이용해서 exploration을가exploitation이 
        - 가이드 방법
            - upper confidence bounds가 있는데 이럴 경우, Bayesian UCB가 된다.
            - Probability matching
    - prior knowledge가 정확할 수록 performance가 좋다.
- Bayesian theorem을 알아야 알 수 있는 내용들이다.
    - posterior: bayes 규칙을 이용해서 데이터와 prior로 계산되는 이후의 분포를 posterior라고 한다. prior와 데이터가 고려되서 만들어진 교정된 분포이다.
- 예컨데 각 bandit machine들은 서로 independent한 gaussian 분포이다. 즉 1번 machine을 당긴다고 2번 machine의 확률분포가 바뀌지 않는다.
    - gaussian 분포를 prior라고 설정하고, 실제 몇번 당겨서 데이터를 보고 posterior를 구하는 방식을 이용해서 신뢰구간을 구할 수 있다.
    - 이를 이용하면 bayesian UCB가 된다.



## Probability Matching
- A가 제일 좋을 확률을 그대로 뽑는 방법론
- Probability matching의 한 예시가 Thompson sampling 이란 방법론이 있다.



## Thompson sampling
- 정말 간단한 Bayesian 방법론이지만, 잘 된다.
- prior와 데이터를 통해서 알게된 reward의 확률분포가 있어서, 각각의 bandit machine에 대해 샘플링을 한다고 하자. 샘플링한 결과로 1번 machine에서는 10만원, 2번 machine에서는 25만원, 3번 machine에서는 10만원이 나왔다면, 2번 machine을 당긴다.
    - 즉 샘플링을 하면 자연스럽게 exploration과 exploitation이 자연스럽게 trade off하면서 값이 나온다.
- probability matching의 한 구현방법
    - 먼저 bayse rule을 이용해서 posterior를 구해서 reward에 대한 분포를 구한다. 즉 각 machine의 reward는 어떤 분포를 따를 것을 구한다. 그리고 이 분포에서 샘플링을 하면 각 machine별 값이 나올 텐데, 이 값 중 제일 높은 action을 선택한다.
    - 만약 샘플링을 1억번 하면, 무조건 posterior 분포의 평균만 본다는 뜻이 되므로, 아마 1번만 샘플링을 하는 게 아닐까 싶다.
    - posterior 분포는 내가 알고 있는 정보를 바탕으로 추측한 확률 분포이다. 시뮬레이션에 쓰이는 확률분포
        - 한번 action을 선택하고 나온 reward를 바탕으로 다시 posterior 분포가 업데이트된다. 그러먼 다시 새로운 분포에서 샘플링을 하나씩 해서 그 중 가장 큰 값이 나온 bandit을 당긴다.
- 베르누이 bandit일 때는 thompson sampling이 Lai and Robbins lower bound(log 함수)를 만족한다.
    - 베르누이 bandit은 값 2개 중에 하나가 나오는 bandit들인데, 다른 bandit에서는 잘 안되지 않을까?
        - 최근 많은 연구 결과에 따르면, 베르누이 bandit 뿐만 아니라 다른 bandit에서도 좋긴 한데, 애초에 baysian 방법론이기 때문에 어떤 magical source에 의해서 prior이 좋아야 잘 작동한다.



## Value of Information
- exploration은 결국 정보를 얻기 위해서 하는 행동이다.
- 그러면 우리가 매번 얻는 정보에 대해 가치를 매길 수 있을까? 얻은 정보에 대해서 계량화할 수 있을까?
    - uncertain 상황에서 얻는 정보의 gain이 크므로, exploration을 더 많이 한다.
- 우리가 정보의 가치를 알 수 있다면, exploration과 exploitation을 optimal하게 trade off할 수 있다.



## Information State Space
- state space에 정보를 더한다. 말하자면, one-step MDP에서 action과 reward만 있었는데, 여기에 information space \tilde{s}를 추가한다.
    - \tilde{s}: history가 있으면 그것으로 만들어진 함수를 통해서 나온 값
        - 지금까지 쌓아놓은 정보를 summarise 해주는 숫자들을 말하며, 이 숫자들을 state로 본다.
        - 내가 어떤 방에 있어서 왼쪽으로 갈지 오른쪽으로 갈지 고민 중인데, 한번은 왼쪽 문을 열어봤을 때 그 방에 있는 것과, 왼쪽을 열어본 적이 없는데 그 방에 있는 것을 서로 다른 state로 본다는 뜻이다. 사실 방이라는 같은 state에 있는 것인데, 내가 그 state에 있을 때 알고 있는 정보를 계산해서 그 정보를 state에 붙여준다(concat). 이렇게 되면 같은 state에 있더라도 알고 있는 정보에 따라 다른 state가 된다.
        - 그러면 왼쪽 문을 3번 열어봤고, 오른쪽 문을 5번 열어봤으면 (3, 5)가 state의 오른쪽에 딱 붙는다.
    - state의 이동이 있을 때, \tilde{P}가 새롭게 계산된다.
- 이렇게 되면 원래는 one-step 문제인데, MDP로 만들어준다. 왜냐하면 원래는 state가 없었는데, information state가 생겨나면서 action과 reward에 \tilde{s}, \tile{P}, \gamma가 추가되면서 MDP가 된다.
    - 이전에 배운 MDP를 활용해서 문제를 푼다.



## Example: Bernoulli Bandits
- reward가 베르누이분포를 통해서 정해진다. 즉 이기거나, 지는 경우만 있다.
    - ex. 이길 확률 \mu_a = 0.5, 꼭 0.5일 필요는 없다.
    - \mu_a가 제일 높은 machine을 찾고 싶다.
- Information state 정의 방법
    - \alpha_a: a를 당겼을 때, 졌던 경우의 횟수
    - \beta_a: a를 당겼을 때, 이긴 경우의 횟수
    - 사실 베르누이분포는 이 count가 모든 history를 기록해주는 것이기 떄문에, 여기서 더 있는 정보가 없다.
        - 따라서 베르누이 bandit문제가 간단한 예시이다.



## Solving Information State Space Bndits
- 이제 우리에게는 information state를 이용한 어떤 MDP가 생겼다. 이 MDP는 reinforcement learning으로 풀 수 있다.
    - Model-free reinforcement learning
    - Bayesian model-based reinforcement learning



## Bayes-Adaptive Bernoulli Bandits
- Beta 분포로 prior를 만들어서 정해놓고, 매던 action을 할때마다 posterior를 업데이트 한다.
    - Bayes rule을 이용한다.
- posterior가 만들어지고, 그 posterior 분포 자체가 information state가 된다. 매번 transition 할 때마다 그것을 이용해서 bayesian model을 업데이트한다.



## 여기에는 없는 정리 슬라이드
- Random Exploration: e-greedy
- Optimism in the Face of Uncertainty
    - 잘 동작하지 않는 예외가 되는 상황
        - action space가 infinite할 경우, 모든 것이 uncertainty하기 때문에 평생 exploration할 것이다. 또는 총 10번만 시도할 수 있는데, 선택지가 그것보다 많을 경우 적당히 exploitation해야 할 것이다. 이 경우 uncertainty를 낙관적으로 보면 안될 것이다.
        - 혹은 real robot이어서, 어떤 실패에 대한 리스크가 너무 커서, 실패를 가능한 피하고 싶을 수 있다. 이 경우 uncertainty를 최대한 안 가보기 위해 exploration을 최대한 안전한 공간에서 해야 한다.(safe exploration)
            - ex. 실제 헬리콥터를 조종하는 문제
- Information State Space
- exploration, exploitation을 trade off하는 여러 알고리즘
    - multi-armed bandit에서 적용



## Contextual Bandits
- A, R에 state가 포함된다.
- 예를 들어, 사람들에게 광고를 띄어주는데, 한번만 띄어주는 것이 아니라, 한번 띄어줘서 무언가를 클릭하면 그 다음에는 이 정보를 바탕으로 다른 광고를 띄어준다. 이렇게 계속 이어지는 multi-armed bandit을 contextual bandit 문제라고 한다.
    - 이 사람이 클릭해온 history가 있는, 맥락이 있는 bandit 문제
- 이보다 더 큰 범위의 문제로 MDP가 있다.
    - MDP: full 강화학습 문제
- 강화학습에서도 multi-armed bandit에서 배운 이론들을 거의 그대로 쓸 수 있다.
    - policy가 계속 바뀌면, 우리가 방문했던 state 정보를 어떻게 담느냐에 따라서 좀 어렵다고 한다.


## Conclusion
