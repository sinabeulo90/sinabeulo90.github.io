---
layout: post
title:  "Lecture 9: Exploration and Exploitation"
date:   2021-04-21 13:57:27 +0900
category: "Reinforcement Learning"
tags: YouTube
plugins: mathjax
---

David Silver 님의 [Introduction to reinforcement learning](https://youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) 강의 내용을 [팡요랩 Pang-Yo Lab](https://www.youtube.com/channel/UCwkGvF7xKz2E0Lv-fZ9wv2g)의 [강화학습의 기초 이론](https://youtube.com/playlist?list=PLpRS2w0xWHTcTZyyX8LMmtbcMXpd3s4TU)에서 풀어서 설명하기에, 이를 바탕으로 공부하여 정리한 내용입니다.

- Video: [[강화학습 9강] Exploration and Exploitation](https://youtu.be/nm6RwuA_pGE)
- Slide: [Lecture 9: Exploration and Exploitation](https://www.davidsilver.uk/wp-content/uploads/2020/03/XX.pdf)


---

## Introduction

### Exploration vs. Exploitation Dilemma

- 실시간으로 의사결정을 할 때, 근본적인 2가지 문제가 있다.
    1. Exploitation: 이미 알고 있는 정보를 바탕으로 가장 좋은 행동을 선택한다. 정보를 수집하기 위해서는 exploration 한다.
    2. Exploration
- 어떤 결정을 내릴 때, 어쩔 때는 정보를 더 수집하기 위해, 알고 있는 정보 중 가장 좋은 행동이 아님에도 실험적인 행동을 선택할 수 있고, 또 어쩔 때는 모인 정보를 바탕으로 가장 좋은 행동을 선택할 수 있다.
    - 본질적으로 이 둘은 동시에 만족할 수 없기 때문에, 이 둘은 trade-off인 관계이다.
    - 결국 최적의 policy를 갖는 agent를 만들고 싶은데, 이를 위해 이 2가지 행동을 꾸준히 선택해야한다.


### Example

- Restaurant Selection
- Banner Advertisements
- Oil Drilling
- Game Playing
- Exploitation과 exploration은 강화학습에서 근본적인 문제이며, 우리 삶에서도 자주 맞닥뜨리는 문제이다.


### Principles

- Exploitation과 exploration 사이의 균형을 어떻게 맞출 수 있을까?
    1. Naive Exploration: $\epsilon$-greedy
        - Add noise to greedy policy (e.g. $\epsilon$-greedy)
        - 굉장히 naive하고 단순하지만, 굉장히 강력한 방법이다.
        - $\epsilon$이라는 작은 확률로 랜덤하게 행동을 선택하고, 그 외에는 greedy하게 내가 알고 있는 정보를 바탕으로 최적의 행동을 선택한다.
        - Ex: Greedy policy에 random성 noisy를 추가하여 행동을 선택한다.
        - Ex: Softmax를 통해 확률분포로 행동을 선택한다.
    2. Optimistic Initialisation
        - Assume the best until proven otherwise
    3. Optimism in the Face of Uncertainty
        - Prefer actions with uncertain values
        - 좀더 체계적이고, Systematic한 접근법이다.
        - 불확실한 value를 갖는 행동을 오히려 좋은 행동으로 생각하는 방법론이다.
    4. Probability Matching
        - Select actions according to probability they are best
    4. Information State Search
        - Lookahead search incorporating value of information
        - 가장 정확한 방법이지만, 계산하기 어려운 방법론이다.
        - State의 가치에 대해 어느정도 확신하고 있는지를 포함해서, 행동을 선택하는 방법론이다.


---

## Multi-Armed Bandits


### The Multi-Armed Bandit

- Multi-armed bandit을 하는 이유는 MDP보다 훨씬 간단한 문제이기 때문이다. 먼저 아이디어를 간단한 문제에 적용해 본다음에, 점점 큰 문제로 확장해 나갈 것이다.
    - MDP: $\langle \mathcal{S} \mathcal{A}, \mathcal{P} \mathcal{R} \gamma \rangle$로 정의된다.
        - $\mathcal{S}$: State space
        - $\mathcal{P}$: State transition probability
    - Multi-armed bandit: $\langle \mathcal{A}, \mathcal{R} \rangle$로 정의된다.
        - Ex: 카지노의 각 bandit machine마다 reward에 대한 자신만의 확률분포와 기댓값이 있다고 하자. 즉, 어떤 machine은 꾸준히 10만원씩 줄 수 있고, 또 어떤 machine은 평균 100만원씩 주는데 variance가 0 ~ 1000으로 클 수도 있다. 이 문제가 간단한 이유는 machine을 하나 정해서 slot을 당기기만 하면 게임이 끝나므로, state가 없기 때문이다. 따라서 플레이어가 선택할 수 있는 action과 해당 action을 선택했을 때의 reward만 있으면 multi-armed bandit 문제가 정의되므로, multi-armed bandit을 1-step MDP라고 부르기도 한다.
- $m$개의 선택할 수 있는 bandit machine이 있고, 각 machine이 갖고있는 reward의 확률분포를 알 수 없으므로, 매번 어떤 machine을 선택하면서 이 확률분포를 알아나가야 한다.
    - $\mathcal{R^a}(r) = \mathbb{P}[r \| a]$
- 매 step마다 하나의 machine을 당기면 각 machine에서 reward를 준다.
    - $\begin{aligned}
        a_t &\in \mathcal{A} \newline
        r_t &\sim \mathcal{R^{a_t}}
    \end{aligned}$
    - *우리의 목표는 cumulative reward $\sum_{\tau=1}^t r_\tau$를 maximise하는 것이다.*{: style="color: red"}
        - 한 번 당겼을 때의 reward를 최대화하는 것이 아닌, 1,000만원의 예산이 있을 때, 전체 보상을 최대화하는 것이다.
- Exploration과 exploitation의 trade-off를 다루기 좋은 문제이다.
    - 어떤 machine의 slot을 당겼을 때 reward가 잘 나오면 해당 machine의 slot만 당길 수도 있지만, 당겨보지 않은 machine에서 더 좋은 reward가 나올 수도 있기 때문에 가끔은 다른 machine의 slot도 당겨봐야 할 것이다.


### Regret


#### Regret

- 그럼 후회는 어떻게 정의할 수 있을까?
    - Action-value $Q(a)$: 어떤 action을 선택했을 때, 해당 bandit machine이 주는 reward의 기댓값
        - $Q(a) = \mathbb{E}[r \| a]$
    - Optimal value $V^\*$: 선택할 수 있는 bandit machine 중 가장 큰 reward 기댓값
        - $V^\* = Q(a^\*) = \max_{a \in \mathcal{A}}Q(a)$
    - Regret: 어떤 action을 선택했을 때, 해당 bandit machine의 reward 기댓값과 optimal value $V^\*$의 차이
        - $l_t = \mathbb{E} [V^\* - Q(a_t)]$
        - 각 action별로 정의되는 개념이다.
        - Optimal value $V^\*$를 모르기 때문에, 정확한 regret를 정확히 알 수 없다.
    - Total regret
        - $L_t = \mathbb{E} \left[ \sum_{\tau=1}^t V^\* - Q(a_\tau) \right]$
        - Ex: Bandit machine들을 100번 선택해서 당겼을 때, 100개의 regret을 모두 더한 값이다.
        - 즉, cumulative reward를 최대화하는 것은 total regret를 최소화하는 것과 같은 의미이다.
- 그냥 cumulative reward를 maximize하면 되는데, 왜 regret라는 개념을 도입해서 새로 접근할까?
    - 어떤 알고리즘을 사용했을 때 왜 학습을 잘하는지 이해하는데 도움이 되기 때문이다.


#### Counting Regret

- Count $N_t(a)$: Action $a$를 선택한 횟수
- Gap $\Delta_a$: Bandit machine 중 가장 높은 reward 기댓값과 해당 bandit machine의 reward 기댓값 차이
    - $\Delta_a = V^\* - Q(a)$
- Regret은 gap과 count에 대한 함수로 정의할 수 있다.
    - $\begin{aligned}
        L_t &= \mathbb{E} \left[ \sum_{\tau = 1}^t V^\* - Q(a_\tau) \right]     \newline
            &= \sum_{a \in \mathcal{A}} \mathbb{E} [N_t(a)] (V^\* - Q(a))       \newline
            &= \sum_{a \in \mathcal{A}} \mathbb{E} [N_t(a)] \Delta_a
        \end{aligned}$
    - *좋은 알고리즘은 큰 gap에 대해서는 적은 count여야 한다. 하지만 문제는 gap을 알 수 없다.*{: style="color: red"}


#### Linear or Sublinear Regret

![Linear or Sublinear Regret](/assets/rl/linear_or_sublinear_regret.png)

- x축: Bandit machine을 당긴 횟수
- y축: Total regret(Gap의 기댓값)
- 어떤 알고리즘은 5% 확률로 평생 exploration 한다고 가정하면, linear한 total regret line이 그려진다.
    - 항상 5%의 확률로 바보같은 행동을 할 수 밖에 없기 때문에, 해당 machine의 reward 기댓값에 대한 gap이 계속 더해진다.
- 어떤 알고리즘이 평생 exploration 하지 않는다고 가정(greedy)해도, linear한 total regret line이 그려진다.
    - 운 좋게 제일 좋은 action을 greedy하게 선택하지 않는 이상, 선택한 machine의 reward 기댓값에 대한 gap이 계속 더해진다.
    - 즉, greedy 알고리즘이 sub-optimal action에 빠지게 되면, linear한 total regret line이 그려진다.
- 그렇다면, exploration과 exploitation을 적절하게 선택하는 알고리즘을 만들어서 multi-armed bandit을 수행한다면, sublinear한 total regret line을 그릴 수 있지 않을까?
    - 그릴 수 있다.


<br>


### Greedy and ε-greedy algorithms


#### $\epsilon$-Greedy Algorithm

- $\epsilon$의 확률로 무작위 행동을 선택하므로, regret가 계속 쌓이게 된다.
    - $l_t \geq \frac{\epsilon}{\mathcal{A}} \sum_{a \in \mathcal{A}} \Delta_a$
    - linear한 total regret line이 그려진다.


#### Optimistic Initialisation

- 모든 $Q(a)$를 maximum으로 초기화한다.
    - 즉, 모든 $Q(a)$를 high value로 초기화하고, Monte-Carlo 방법으로 action value를 업데이트 한다.
        - $\hat{Q}\_t(a_t) = \hat{Q}\_{t-1} + \frac{1}{N_t(a_t)}(r_t - \hat{Q}\_{t-1})$
    - 매우 간단하면서, 실용적인 아이디어 이다.
    - Ex: Bandit machine이 모두 1억원의 기댓값을 가지고 있다고 초기화하고, greedy하게 machine을 선택한다고 하자. 처음에는 random하게 machine을 선택할 것이다. 선택한 machine에서 100만원이 나왔다면, 해당 machine의 value는 1억과 100만원의 평균이 될 것이다. 그리고 다른 machine을 선택했을 때 0원이 나왔다면, 해당 machine의 value는 1억과 0원의 평균이 될 것이다. 이를 반복하게 되면, 처음에는 모든 machine들이 높은 value를 갖고 있다가, 한번 당겨질 때마다 value가 낮아지고, 이후에는 당기지 않은 machine들을 먼저 당기게 될 것이다. 시간이 지나, 어느정도 value가 수렴하게 되면 높은 value를 갖는 machine만을 선택하게 된다.
        - 매 step마다 action을 선택할수록, 높은 value의 machine이 실제 value scale에 수렴하도록 충분히 exploration 한다.
- 초기 단계에서 exploration을 권장하지만, 이 방법이 제일 좋은 action으로 수렴한다는 보장이 없다.
    - 운이 나쁘면, 3등 정도하는 value의 machine에서 좋은 reward가 나와서, 해당 machine만을 당기게 될 수도 있다.
    - Greedy + optimistic initialization: Linear total regret
    - $\epsilon$-greedy + optimistic initialization: Linear total regret
- 이 방법은 초기화하는 방법론이기 때문에, greedy 또는 $\epsilon$-greedy 모두 사용할 수 있다. 이 2가지 greedy를 사용해도 linear total regret을 벗어나지 못하지만, 이 방법은 충분히 좋은 아이디어이고 실무에 적용할 때도 좋은 아이디어라는 인상을 가졌으면 한다.


#### Decaying $\epsilon$-Greedy Algorithm

- 어떤 스케쥴을 갖고 $\epsilon$을 점점 줄여나간다.
    - 직관적으로는, 처음은 좀 높은 $\epsilon$을 갖고 exploration 하다가, 어느정도 정보가 쌓이면 하나의 행동으로 수렴하기 위해 $\epsilon$을 줄여나간다.
    - 여러가지 스케쥴을 만들 수 있겠지만, 여기서는 heuristic한 스케쥴을 예시를 든다.
        - $\begin{aligned}
            c &> 0      \newline
            d &= \min_{a \| \Delta_a > 0} \Delta_i      \newline
            \epsilon_t &= \min \left\\{ 1, \frac{c |\mathcal{A}}{d^2t} \right\\}
        \end{aligned}$
            - $d$: Bandit machine들 중 가장 높은 value와 그 다음으로 높은 value의 차이
                - $d$가 작으면 분모가 작아지므로, $\epsilon$이 커진다.
                - $d$가 커지면 분모가 커지므로, $\epsilon$이 작아진다.
                - 즉 두 value의 차이가 작을수록 exploration 해보고, 차이가 많이 날수록 exploitation 한다.
            - $c$: 충분히 작은 상수값
            - Action space가 클수록 $\epsilon$이 커진다.
            - $t$가 분모에 있기 때문에 매 step 진행될수록, $\epsilon$이 줄어든다.
- *Decay $\epsilon$-greedy의 total regret은 log함수 형태를 보인다.*{: style="color: red"}
    - *실제 gap을 모르기 때문에, 위 예시에서는 여러 heuristic과 $d$를 사용하여 $\epsilon$을 줄여나갔다.*{: style="color: red"}
        - *만약 heuristic을 잘못 정의해서 스케쥴을 정의하게 되면 catastrophic하게 망한다.*{: style="color: red"}
    - *따라서 우리의 목적은 multi-armed bandit에서 $\mathcal{R}$에 대한 정보 없이도, sublinear regret을 만드는 알고리즘을 찾아내야 한다.*{: style="color: red"}


<br>


### Lower Bound


#### Lower Bound

- 어떤 알고리즘도 깨지 못하는 lower bound가 존재하고, 이 lower bound는 log함수이다. 즉 어떤 천재적인 알고리즘도 log함수보다 더 아래로 내려갈 수 없다.
    - Lower bound: 아무리 잘해도 더 잘할 수 없는 performance 한계치를 의미한다.
- Multi-armed bandit 문제도 이미 lower bound performance가 있고, 이 performance는 가장 높은 기댓값을 갖는 machine과 다른 machine의 유사도에 의존하므로 이에 따라 문제의 난이도가 결정된다.
    - 굉장히 비슷한 확률분포를 가질수록 어려운 문제이고, 비슷한 정도의 분포를 계산하는데 KL divergence를 이용할 수 있다.
        - KL divergence: 어떤 두 확률분포 사이의 차이를 나타내는 값이다. 차이가 클수록 값이 커지고, 유사한 분포일수록 값이 작아진다.
    - 각 bandit machine의 reward 평균은 다르지만 분포는 비슷하다. 따라서 계속 각 machine들을 당겨봐야하는데, 그 과정에서 계속 regret이 쌓이게 된다.
- Theorem (Lai and Robbins)
    - $\lim_{t \rightarrow \infty} L_t \geq \log t \sum_{a \| \Delta_a > 0} \frac{\Delta_a}{KL(\mathcal{R^a} \|\| \mathcal{R}^{a^\*})}$
        - Total regret은 각 action의 gap과 해당 action의 KL divergence로 정의된다.
            - KL divergence: 해당 machine의 확률분포와 가장 큰 가치를 갖는 machine의 확률분포가 얼마나 비슷한지를 수치로 나타낸 값
            - 확률분포가 비슷할수록 KL divergence가 작아지므로 우변의 값이 커지므로 문제가 어려워진다. 또한 gap이 클수록 문제가 어려워진다. 즉 알고리즘이 도달할 수 있는 lower bound의 한계치가 높아진다는 의미이다.
- 정리하면, 알고리즘을 아무리 잘 설계해도 log함수보다 더 성능이 좋아지지는 않는다.
    - log함수는 $t$에 대해 단조증가 함수이므로, regret가 상수로 수렴하는 것은 이론적으로 불가능하다.


<br>


### Upper Confidence Bound


#### Optimism in the Face of Uncertainty

![Optimism in the Face of Uncertainty](/assets/rl/optimism_in_the_face_of_uncertainty_1.png)

- 3개의 bandit machine이 있고, 각 machine의 기댓값이 위와 같은 신뢰구간으로 표현했다고 하자.
    - 이 때, 우리는 초록색 machine이 높은 신뢰구간에 있기 때문에 당겨보고 싶을 것이다.
- Optimism in the face of uncertainty는 uncertainty를 긍정적으로 바라보자는 철학이 있다.
    - 파란색 machine의 95% 신뢰구간을 보면, $Q$의 오른쪽 끝이 매우 크다는 것을 알 수 있다. 즉 현재 파란색 machine에 대해서는 uncertainty가 굉장히 높으므로, 더 explore 해야 하는 중요한 action이 된다. 그리고 이 파란색 machine이 best action이 될 수 있음을 기대한다.


#### Optimism in the Face of Uncertainty (2)

![Optimism in the Face of Uncertainty](/assets/rl/optimism_in_the_face_of_uncertainty_2.png)

- 파란색 machine을 선택했을 때, 이 때의 reward가 좋지 않았으므로 파란색의 평균이 왼쪽으로 이동했고, 불확실성도 낮아지면서 파란색의 신뢰구간도 줄어들었다.
- 이제 다시 제일 가치있는 machine을 선택할 때, 다른 machine을 선택할 가능성이 높아졌다.


<br>

### Upper Confidence Bound


#### Upper Confidence Bounds

- 각 action value의 신뢰구간(Confidence interval)이 있을 때, upper confidence가 있을 것이다.
- 그리고, 실제 value $Q(a)$는 추정치 $\hat{Q}\_t(a) + \hat{U}\_t(a)$보다 작을 확률이 높을 것이다.
    - $Q(a) \leq \hat{Q}\_t(a) + \hat{U}\_t(a)$
    - Ex: 95% 신뢰구간이라는 뜻은 $Q(a)$가 추정치 $\hat{Q}\_t(a) + \hat{U}\_t(a)$보다 작을 확률이 95%라는 의미이다. 즉, 우리가 생각하는 구간에 $Q(a)$가 존재할 확률이 95%라는 의미이다.
- $\hat{Q}$는 샘플들이 많을수록 점점 더 정확해 질 것이다.
- $\hat{U}$는 $N_t(a)$가 작을수록 신뢰구간이 커지고 그만큼 정확도가 낮을 것이고, $N_t(a)$가 커질수록 신뢰구간이 작아지고 그만큼 정확도가 높을 것이다.
- 위의 성질을 이용해서 upper confidence bounds를 최대화하는 action을 선택할 수 있다.
    - $a_t = \arg\max_{a \in \mathcal{A}} \hat{Q}\_t(a) + \hat{U}\_t(a)$
    - Frequentist view
    - Bayesian approach


#### Hoeffding’s Inequality
{: style="color: red"}

- 모분포를 몰라도, frequentist view를 사용할 수 있는데, 이 방법을 사용하기 위해 Hoeffding’s inequality이 필요하다.
- Hoeffding’s inequality
    - 0과 1사이의 random variables $X_1, \dots, X_t$가 있을 때, 이들의 sample mean을 $\overline{X}$라고 하면, 아래와 같은 부등식이 성립한다.
        - $\begin{aligned}
            & \overline{X}\_t = \frac{1}{\tau} \sum_{\tau = 1}^t X_t   \newline
            & \mathbb{P} \left[ \mathbb{E}[X] > \overline{X}\_t + u \right] \leq e^{-2tu^2}
        \end{aligned}$
            - $\mathbb{E} [X]$: $X$의 실제 모평균으로, $X$의 기댓값이다.
            - $u$: 틀리는 정도, 오차 값
            - $t$: 샘플링한 갯수
            - $X$의 기댓값과 샘플 평균이 $u$만큼 차이날 확률은 $e^{-2tu^2}$보다 작다는 것이 이미 증명되어 있다.
            - 직관적으로 생각하면, $u$는 틀린 정도이므로 $u$가 클 수록 확률은 작아질 것이다.
                - Ex: $u$가 10만큼 틀릴 확률은 1만큼 틀릴 확률보다 낮으므로 우변에 $-u^2$이 붙어있다. 또 100개의 샘플 평균이 10개의 샘플평균보다 더 정확하므로 우변에 $-t$가 붙어있다.
    - 모분포와 관계없이 항상 사용할 수 있는 부등식이다.
    - UCB의 개념을 적용할 수 있는 부등식은 정말 많은데, 그 중 하나이다.
        - Ex: Bernstein's inequality, Azuma's inequality 등
- 이 부등식을 multi-armed bandits에 적용하면, action $a$를 선택할 때의 기댓값은 $Q(a)$이고, 샘플평균은 $\hat{Q}\_t(s)$이 되므로, upper confidence bound $U_t(a)$는 아래와 같다.
    - $\mathbb{P} \left[ Q(a) > \hat{Q}\_t(a) + U_t(a) \right] \leq e^{-2N_t(a)U_t(a)^2}$


#### Calculating Upper Confidence Bounds

- $\begin{aligned}
    e^{-2N_t(a)U_t(a)^2} &= p   \newline
    U_t(a)               &= \sqrt{\frac{-\log p}{2N_t(a)}}
\end{aligned}$
    - Upper confidence bounds를 계산할 때, $Q(a)$가 UCB를 초과할 $p$를 적당한 값으로 정해서 푼다.
        - Ex: $p = 0.95$: 95%의 확률로 맞추고 싶다.
    - $p$를 정하면, 각 action 별로 $Q$의 추정치가 계산되는데, 여기서 $U_t(a)$를 더했을 때 가장 큰 값을 가지는 action을 선택한다.
- *$p$를 고정하기보단, $p = t^{-4}$라고 스케쥴로 정의해서 진행하면, $U_t(a)$의 log에 $t$만 남고, 4가 빠져나오면서 아래와 같은 식이 되고 UCB1 알고리즘이라고 말한다.*{: style="color: red"}
    - $U_t(a) = \sqrt{\frac{2\log t}{N_t(a)}}$


#### UCB1

- UCB1 algorithm
    - $a_t = \arg\max_{a \in \mathcal{A}} Q(a) + \sqrt{\frac{2\log t}{N_t(a)}}$
        - 각 action의 $N_t(a)$와 $Q(a)$를 통해, $Q(a) + \sqrt{\frac{2\log t}{N_t(a)}}$가 가장 큰 값을 가지는 action을 선택한다.
- Theorem
    - $\lim_{t \rightarrow \infty} L_t \leq 8 \log t \sum_{a \| \Delta_a > 0} \Delta_a$
        - 결국 행동을 선택하기 위해 upper confidence bound를 계산하려고 하는데, 모분포를 모를 경우 Hoeffding’s 부등식을 활용해서 upper confidence bound를 계산한다.
        - 그리고 가장 큰 $Q + U$를 가지는 action을 선택하면 exploration과 exploitation을 잘 하면서 total regret은 log함수 형태를 보인다.


#### Example: UCB vs. ε-Greedy On 10-armed Bandit

![UCB vs. ε-Greedy On 10-armed Bandit](/assets/rl/ucb_vs_epsilon_greedy.png)

- 10개의 Bandit machine이 있고, 각 machine의 reward는 같지만 확률이 다를 때, $\epsilon$-greedy와 UCB의 performance를 비교하였다.
    1. Reward를 줄 확률: 90% 1대, 60% 9대
    2. Reward를 줄 확률: 90% 1대, 80% 3대, 70% 3대, 60% 3대
- *놀랍게도 $\epsilon$-greedy가 꽤 잘하지만, $\epsilon$을 잘못 정의하면 망한다.*{: style="color: red"}
    - 우상단 그래프에서 $\epsilon = 0.05$인 경우, regret이 올라가면서 망한 것을 알 수 있다.
    - $\epsilon$을 잘 설정한다면, UCB보다 좋은 경우도 많다.
- 반면 UCB는 항상 잘 동작한다.
    - 거의 모두 가장 좋은 machine을 선택한다.
- 그렇다면 UCB는 왜 $\epsilon$-greedy와 비슷한 성능을 낼까?
    - Hoeffding's 부등식은 모분포에 대해 가정하는 것이 거의 없기 때문에, 굉장히 약한 부등식이다. 심지어 분포가 symmetrical(대칭적)하지 않아도 된다. 다만 여기서 가정하는 것은 reward가 bounded되어 있다고만 가정한다.
        - Hoeffding's 부등식을 보면 0 ~ 1 사이의 값을 갖는다는 가정이 있는데, 이 때, reward는 0 ~ 1 사이의 값이 아니므로 bounded 되어야 max 값으로 나누어 줄 수 있다.
        - Ex: bandit machine의 reward가 아무리 커도 1,000만원 이상은 안나온다는 가정이 있어야 한다. 최댓값을 아주 크게 설정해도, 그 값보다 큰 값이 나올 확률이 있다면 Hoeffding's 부등식은 성립하지 않는다. 최댓값을 넘지 않는다면 실무에서 사용할 수 있을 것이다.
    - 만약 가정이 더 추가된, 좀 더 제한적이고 정밀한 부등식을 사용한다면 더 효율이 올라갈 수 있을 것이다.
        - 그만큼 좀 더 많은 정보가 필요할 것이다.


<br>


### Bayesian Bandits


#### Bayesian Bandits

- Bayesian Bandits은 reward distribution에 대한 prior knowledge $p[\mathcal{R}]$를 사용한다.
    1. Bandit machine을 당겨서, prior knowledge를 통해 나온 reward를 보고, reward의 history $h_t$를 통해 reward의 posterior 분포 $p[\mathcal{R} \| h_t]$를 구한다.
        - $h_t = a_1, r_1, \dots, a_{t-1}, r_{t-1}$
        - Prior knowledge는 사람이 정의한다.
            - Ex: Bernoulli 분포, Gaussian 분포
        - Posterior: Bayes 규칙을 이용해서, prior 분포와 데이터를 고려해서 만들어진 교정된 분포이다.
    2. Posterior 분포를 통해 exploration을 진행한다.
        - Upper confidence bounds (Bayesian UCB)
        - Probability matching (Thompson sampling)
        - Prior knowledge가 정확할수록 performance가 좋다.
- 이는 Bayesian theorem을 알아야 이해할 수 있다.
    - Ex: 각 bandit machine은 서로 independent한 Gaussian 분포이다. 즉, 1번 machine을 당긴 행동이 2번 machine의 확률분포에 영향을 미치지 않는다. 이 때, Gaussian 분포를 prior로 설정하고, 실제로 machine을 당겨서 나온 reward를 통해 posterior를 계산할 수 있다. 그러면 이 posterior로 신뢰구간을 구할 수 있게 된다. 이를 이용한 UCB가 Bayesian UCB이다.


#### Probability Matching

- Reward의 history $h_t$를 통해 가장 좋은 행동을 선택한다.
    - $\pi(a \| h_t) = \mathbb{P} [Q(a) > Q(a'), \forall a' \neq a \| h_t]$
    - Probability matching은 불확실성에 대해 낙관적적이다.
        - 불확실한 행동은 선택하는 것이 가장 좋은 reward를 얻을 확률을 높다.
        - Ex: Thompson sampling


#### Thompson Sampling

- Probability matching을 구현한 것이다.
    - $\begin{aligned}
        \pi(a \| h_t) &= \mathbb{P} [Q(a) > Q(a'), \forall a' \neq a \| h_t]    \newline
                      &= \mathbb{E_{\mathcal{R} \| h_t}} \left[ \mathbb{1}(a = {\arg\max}_{a \in \mathcal{A}} Q(a) \right]
    \end{aligned}$
        - 정말 간단한 Bayesian 방법론이지만 잘된다.
- Probability matching의 구현 방법 예시
    1. 먼저 Bayse law를 통해 posterior를 계산해서 reward에 대한 분포 $p[\mathcal{R} \| h_t]$를 구한다.
        - Posteriro 분포는 현재 알고있는 정보를 바탕으로 추측한 확률분포이고, 이 분포는 시뮬레이션을 하기 위해 사용된다.
    2. 각 machine의 reward 분포를 따르는 샘플링을 한다.
    3. Action-value function을 계산한다.
        - $Q(a) = \mathbb{E} \left[ \mathcal{R_a} \right]$
    4. 이 중 가장 큰 value를 가지는 machine을 선택한다.
        - $a_t = {\arg\max}_{a \in \mathcal{A}} Q(a)$
        - Ex: 1번 machine에서 10만원, 2번 machine에서 25만원, 3번 machine에서 10만원이 나왔을 때, 2번 machine을 당긴다.
    5. 어떤 action을 선택하고 나온 reward를 바탕으로 다시 [1]로 돌아가서, posterior를 계산해서 reward에 대한 분포 $p[\mathcal{R} \| h_t]$를 업데이트 한다.
- Thompson sampling을 하면 자연스럽게 exploration과 exploitation을 반복하게 된다.
- Bernoulli bandit인 경우, Thompson sampling을 하면 Lai and Robbins lower bound(log 함수)를 만족한다.
    - Bernoulli bandit은 값이 2개 중 하나만 나오는 bandit인데, 다른 종류의 bandit에서도 잘 될까?
        - 최근 많은 연구 결과에 따르면, Bernoulli bandit 뿐만 아니라 다른 종류의 bandit에서도 좋다. 하지만 애초에 Bayesian 방법론을 따르기 때문에, magical source에 의해 prior이 좋아야 잘 동작한다.


<br>


### Information State Search


#### Value of Information

- Exploration은 추가적인 정보를 얻기위한 행동이다.
- *그럼 우리가 얻은 정보에 대해 가치를 매길 수 있을까?*{: style="color: red"}
    - 어떤 결정을 내리기 전에, 그 정보를 얻기 위해 얼만큼의 보상금을 지불할 수 있을까?
    - 그 정보를 얻은 뒤 장기적으로 얻는 이익에 지금 주는 보상금을 뺀 값을 그 정보에 대한 가치로 계산할 수 있지 않을까?
- Uncertain 상황에서 얻는 정보의 가치는 클 것이므로, 이 상황에서는 exploration을 더 많이 한다.
- 만약 정보의 가치를 알 수 있다면, exploration과 exploitation을 optimally하게 trade-off 할 수 있을 것이다.


#### Information State Space

- 1-step MDP는 action과 reward로만 구성되어 있는데, 여기에 information space $\tilde{s}$를 추가한다.
    - $\tilde{s}\_t = f(h_t)$
        - History $h_t$를 입력으로 하는 어떤 함수에서 계산된 값을 의미한다.
        - 현재까지의 축적된 모든 정보를 summarize한 숫자를 의미하며, 이 숫자를 state로 생각한다.
            - Ex: 현재 어떤 방에서 왼쪽 방을 들어갈 것인지, 오른쪽 방을 들어갈 것인지 고민 중이라고 하자. 이 때, 한 번은 왼쪽 방을 들어가본 뒤 고민하는 것과 왼쪽방을 들어가본 적 없는 상태에서 고민하는 것은 서로 다른 state로 본다는 뜻이다. 고민 중인 방에 대한 state를 정의할 때, 왼쪽 방에 들어갔을 때의 경험을 추가(concat)하면, 같은 state에 있더라도 알고 있는 정보에 따라 다른 state가 된다.
                - Ex: 왼쪽 문을 3번 들어갔고, 오른쪽 문을 5번 들어갔으면 state에 (3, 5)를 추가한다.
    - Action을 선택하면 transition probability $\tilde{\mathcal{P}}^a_{\tilde{s}\tilde{s}'}$와 information state $\tilde{s}$가 추가된다.
    - 이렇게 되면, information state space가 포함된 MDP $\tilde{\mathcal{M}}$가 만들어진다.
        - $\tilde{\mathcal{M}} = \langle \tilde{\mathcal{S}}, \mathcal{A}, \tilde{\mathcal{P}}, \mathcal{R}, \gamma \rangle$
        - 이전에 배운 방법들로 MDP를 푼다.


#### Example: Bernoulli Bandits

- Reward가 Bernoulli 분포에 따라 정해진다고 하자.
    - $\mathcal{R}^a = \mathcal{B}(\mu_a)$
    - Ex: 이길 확률 $\mu_a = 0.5$
- 이 때, $\mu_a$가 가장 큰 bandit machine을 찾고자 할 때, 아래와 같이 imformation state를 정의한다.
    - $\tilde{s} = \langle \alpha \beta \rangle$
        - $\alpha_a$: $a$를 당겼을 때, reward 0이 나온 횟수
        - $\beta_a$: $a$를 당겼을 때, reward 1이 나온 횟수
    - Bernoulli 분포는 이 count만으로 모든 history를 기록할 수 있기 떄문에, 간단한 예시로 사용되기 적합하다.


#### Solving Information State Space Bandits

- Information state가 포함된 MDP $\tilde{\mathcal{M}}$을 reinforcement learning으로 풀 수 있다.
    - Model-free reinforcement learning
        - Ex: Q-learning (Duff, 1994)
    - Bayesian model-based reinforcement learning
        - Ex: Gittins indices (Gittins, 1979)
        - Bayes-adaptive RL이라고도 한다.
        - prior 분포를 통해 Bayes-optimal exploration/exploitation trade-off를 찾는다.


#### Bayes-Adaptive Bernoulli Bandits

![Bayes-Adaptive Bernoulli Bandits](/assets/rl/bayes_adaptive_bernoulli_bandits.png)

- 먼저, prior $\text{Beta}(\alpha_a, \beta_a)$ 분포를 정의하고, 매 action을 선택할 떄마다 Bayes rule을 통해 $\mathcal{R^a}$에 대한 posterior가 업데이트 된다.
    - $\begin{aligned}
        \text{Beta}(\alpha_a + 1, \beta_a) \  \text{if } r = 0      \newline
        \text{Beta}(\alpha_a, \beta_a + 1) \  \text{if } r = 1
    \end{aligned}$
- Posterior는 Bayes-adaptive MDP의 transition function $\mathcal{P}$가 된다.
- Information state $\langle \alpha, \beta \rangle$은 reward model $\text{Beta}(\alpha, \beta)$에 해당하므로, 매 state의 transition은 Bayesian model의 업데이트를 의미한다.


#### Slide에는 없는 정리 내용

- Multi-armed bandit에서 exploration과 exploitation을 trade-off하는 여러가지 알고리즘을 배웠다.
    1. Random exploration
        - $\epsilon$-greedy
    2. Optimism in the face of uncertainty
        - 만약, Action space가 무한할 경우, 모든 행동들이 uncertainty하기 때문에 평생 exploration할 것이다. Exploration은 10번만 진행하고, 나머지의 경우는 exploitation을 해야할 수도 있다.
            - Ex: 실제 헬리콥터를 조종하는 문제를 학습시킬 떄, 실패에 대한 리스크가 너무 크기 때문에 가능한 실패를 피하고 싶을 것이다. 이 경우, uncertainty를 최대한 피하기 위해 exploration을 최대한 안전한 공간에서 해야 할 것이다. (Safe exploration)
    3. Information state space


---


## Contextual Bandits

### Contextual Bandits

- Contextual bandit은 multi-armed bandit에 state $S$가 추가된다.
    - $\langle \mathcal{A}, \mathcal{S}, \mathcal{R} \rangle$
    - $\mathcal{A}$: 알고있는 action space이다.
    - $\mathcal{S}$: State에 대한 어떤 모르는 확률분포이다.
        - $\mathcal{S} = \mathbb{P}[s]$
    - $\mathcal{R}$: Reward에 대한 어떤 모르는 확률분포이다.
        - $\mathcal{R_s^a} = \mathbb{P}[r \| s, a]$
    - Ex: 사람들에게 광고를 보여준다고 하자. 이 때, 한 번만 보여주는 것이 아니라, 무언가를 클릭하면 이 정보를 바탕으로 다른 광고를 보여준다. 이 때, 클릭에 대한 history가 있으므로, 맥락이 있는 bandit 문제이다.
- 매 step $t$마다 아래 과정을 따른다.
    1. Environment는 확률분포 $\mathcal{S}$를 따르는 $s_t$를 뽑는다.
        - $s_t \sim \mathcal{S}$
    2. Agent는 action을 선택한다.
        - $a_t \in \mathcal{A}$
    3. Environment는 해당 state $s_t$에 선택된 action $a_t$에 대한 reward $r_t$를 준다.
        - $r_t \sim \mathcal{R_{s_t}^{a_t}}$
- 우리의 목표는 cumulative reward $\sum_{\tau = 1}^t r_t$를 maximize하는 것이다.


---

## MDPs

### Information State Search

#### Conclusion

![Conclusion](/assets/rl/exploration_exploitation_conclusion.png)
