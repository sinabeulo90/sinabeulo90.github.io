---
layout: post
title:  "Chapter 9.2 예측 목적"
date:   2021-01-03 00:00:00 +0900
categories: "Reinforcement Learning"
---

## 9.2 예측 목적($\overline{VE}$)

표 기반

- 학습된 가치 함수가 실제 가치 함수와 정확하게 같아질 수 있다.
- 예측 품질을 연속적으로 측정할 필요 없다.
- 각 상태에서 학습된 가치 갱신은 다른 상태에 영향을 주지 않는다.

근사적 해법

- 한 생태에서의 갱신이 다른 많은 상태에 영향을 미친다.
- 모든 상태에 대해 가치를 정확하게 추정 불가능하다.
- 가중치보다 훨씬 많은 수의 상태를 갖는다고 가정한다.
- 즉, 한 상태의 가치 추정값을 더 정확하게 만드는 것은 언제나 다른 상태를 덜 정확하게 만든다.


### 9.2.1 그럼 근사적 해법은 어떤 상태를 가장 우선시 할 것인가?

각 상태 $s$에 대해 가치 추정의 오차를 줄이는 노력을 얼만큼 기울일 것인지에 대한 상태 분포 $\mu(s)$를 명시해야 한다.

$$ \mu(s) \ge 0, \, \sum_s \mu(s) = 1 $$

상태 $s$의 가치 추정 오차를 $(\text{근사 가치} \, \hat{v}(s, \mathbf{w}) - \text{실제 가치} \, v_\pi(s))^2$로 정의하고, 상태 공간에 걸쳐 이 오차에 가중치 $\mu$를 할당하여 자연스러운 목적 함수를 얻을 수 있다. 이 목적 함수를 평균 제곱 가치 오차<sup>Mean Square Value Error, $\overline{VE}$</sup>라고 하며 아래와 같이 정의된다.

$$ \overline{VE} (\mathbf{w}) \doteq \sum_{s \in S} \mu(s) [v_\pi(s) - \hat{v}(s, \mathbf{w})]^2 $$

오차의 제곱근, 제곱근 $\overline{VE}$는 근사 가치가 실제 가치와 얼마나 차이를 갖는지에 대한 대략적인 지표로 그래프에서 종종 사용된다.

$\mu(s)$는 상태 $s$에서 소비된 시간의 비율로써, 활성 정책 훈련의 경우 활성 정책 분포<sup>on-policy distribution</sup>라고 불린다. 연속적인 문제에서는 정책 $\pi$를 따르는 정상<sup>stationary</sup> 분포를 의미하지만, 에피소딕 문제에서는 초기 상태가 어떻게 선택되는 지에 따라 영향을 받는다. 에피소드가 각 상태 $s$에서 시작할 확률을 $h(s)$, 단일 에피소드에서 상태 $s$가 평균적으로 소비된 시간 단계의 수를 $\eta(s)$라고 할때 모든 상태 $s$에 대해 에피소딕 문제의 활성 정책 분포는 아래와 같다.

$$ \eta(s) = h(s) + \sum_{\bar{s}} \eta(\bar{s}) \sum_a \pi(a|\bar{s})p(s|\bar{s}, a) $$

할인이 있을 경우 ($\gamma < 1$),

$$ \eta(s) = h(s) + \gamma \sum_{\bar{s}} \eta(\bar{s}) \sum_a \pi(a|\bar{s})p(s|\bar{s}, a) $$

위의 연립 방정식을 풀어서 상태를 마주친 횟수에 대한 기댓값 $\eta(s)$ 구해서, 각 상태에서 소비된 시간의 비율을 정규화해서 활성 정책 분포를 알 수 있다.

$$ \text{모든} \, s \in S \, \text{에 대해}, \, \mu(s) = \frac{\eta(s)}{\sum_{s'} \eta(s')} $$

연속적인 문제와 에피소딕 문제 모두 서로 유사하게 작동하지만, 근사에서 학습의 목적을 분명하게 하기 위해 형식적인 분석은 따로 분리되어야 한다.


### 9.2.2 강화학습에서 $\overline{VE}$의 의미

가치 함수를 학습하는 궁극적인 목적은 더 좋은 정책을 찾는 것 이지만, $\overline{VE}$가 올바른 성능 목적이 된다는 것은 사실 완전히 명확하지 않다. 더 좋은 정책을 찾기 위한 가장 좋은 가치 함수가 $\overline{VE}$를 최소화하는 가치함수 일 필요는 없지만, 더 나은 대안이 무엇인지도 아직 명확하지 않다.

여기서는 $\overline{VE}$에 초점을 맞춘다.

$\overline{VE}$ 측면에서 이상적인 목표는 모든 가능한 $\mathbf{w}$에 대해 $\overline{VE}(\mathbf{w}^\*) \le \overline{VE}(\mathbf{w})$를 만족하는 전역 최적값<sup>global optimum</sup> $\mathbf{w}^\*$을 찾는 것이다. 선형 함수와 같은 간단한 함수의 근사에서는 때때로 달성 가능하지만, 인공 신경망이나 결정트리 같은 복잡한 함수의 근사에서는 좀처럼 달성하기 어렵기 때문에, $\mathbf{w}^\*$ 근처의 모든 $\mathbf{w}$에 대해 $\overline{VE}(\mathbf{w}^\*) \le \overline{VE}(\mathbf{w})$을 만족하는 지엽적 최적값<sup>local optimum</sup> $\mathbf{w}^\*$을 찾으려 할 수 있다. 이는 전역 최적값을 보장하진 않지만, 지엽적 최적값은 일반적으로 비선형 함수 근사에서 찾을 수 있는 최선이고, 이것으로 충분한 경우가 많다.

**문제는, 강화학습 분야에서 관심을 갖고 있는 많은 문제들이 전역 최적값 또는 지엽적 최적값으로 수렵한다는 것을 보장할 수 없다는 것이다. 어떤 방법에서는 $\overline{VE}$가 무한으로 발산할 수 있다.**
