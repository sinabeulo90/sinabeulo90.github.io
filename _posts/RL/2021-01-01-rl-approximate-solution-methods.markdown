---
layout: post
title:  "Ch 9 근사를 이용한 활성 정책 예측 ~ Ch 9.1"
date:   2021-01-01 00:00:00 +0900
categories: "Reinforcement Learning"
---

활성 정책 데이터($\pi$)로부터 상태 가치 함수($v_\pi$)를 추정할 때, 함수의 근사를 사용하는 것을 고려한다. 상태 $s$의 근사적 가치를 구하기 위해, 근사적 상태 가치 함수 $\hat{v}_\pi$는 가중치 벡터 $\mathbf{w} \in \mathbb{R}^d$를 갖는 파라미터화된 함수 형태로 표현한다.

$$ \hat{v}(s, \mathbf{w}) \approx v_\pi $$

| $\hat{v}$ | $\mathbf{w}$ |
|:-|:-|
| 상태의 특징에 대한 선형 함수 | 특징들의 가중치 벡터 |
| 다층<sup>multi-layer</sup> 인공 신경망에 의해 계산된 함수 | 모든 층에서의 연결 가중치 벡터 |
| 결정 트리<sup> decision tree</sup>에 의해 계산되는 함수 | 트리의 분기점<sup>split point</sup>과 리프의 가치를 정의하는 모든 숫자 |

표 형식 방법을 사용할 수 없을 만큼 매우 다양한 상태가 있을 경우, 근사적 방법을 사용하므로 가중치의 갯수는 상태의 갯수와 비교하면 매우 적을 수 밖에 없다. 

$$ d \ll |S| $$ 

이는 학습자가 전체 상태에 접근할 수 없는 문제를 강화학습에 적용할 수 있도록 만든다. 파라미터화된 $\hat{v}$의 형태가 가치 추정값으로 하여금 상태의 특정 측면에 의존하지 못하게 하는 것과 유사하므로 강화학습을 부분적으로 관측 가능한 문제에 적용할 수 있다.

하나의 가중치를 바꾸면, 많은 상태의 가치 추정값이 바뀐다. 즉 하나의 상태를 일반화하기 위해 가중치 값이 갱신되면, 다른 상태들의 가치값에 영향이 미치게 된다. 즉 일반화<sup>generalization</sup>는 학습을 잠재적으로 더 강력하게 만들기도 하지만, 한편으론 잠재적으로 관리하고 이해하기가 더 어려워진다.

함수 근사는 상태에 대한 표현을 이전 관측에 대한 기억과 결합하지 못한다. (**가능한 방법: 17.3**)


## 9.1 가치 함수 근사

예측 방법: 특정 상태에서의 가치를 그 상태의 보강된 가치(갱신 목표<sup>update target</sup>)로 이동시키기 위한 가치 함수 추정값 갱신으로 설명할 수 있다.

갱신의 대상이 되는 상태 $s$의 추정값이 갱신 목표 $u$에 더 가까워지도록 이동하는 개별적인 갱신을 아래와 같이 표현한다.

$$ s \mapsto u $$

| 갱신 종류 | $s \mapsto u$ |
|:-|:-|
| 몬테카를로 갱신 | $S_t \mapsto G_t$ |
| $\text{TD(0)} 갱신 | $S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$ |
| $n$단계 $\text{TD}$ 갱신 | $S_t \mapsto G_{t:t+n}$ |
| 동적 프로그래밍 정책 평가 갱신 | 임의의 $s$에 대해, $s \mapsto \mathbb{E}_\pi [ R\_{t+1} + \gamma \hat{v}(S\_{t+1}, \mathbf{w}_t) \| S_t = s]$ |

몬테카를로, $\text{TD}$ 갱신은 실제 경험에서 마주치는 상태 $S_t$에 대해서 동작한다.

위와 같이 갱신하는 방법을 지도학습<sup>supervised learning</sup> 방법이라고 하고, 출력이 $u$와 같은 숫자일 경우 함수 근사<sup>function approximation</sup>라고 한다. 이 방법은 근사하려고 하는 함수의 입출력 관계를 나타내는 예제가 주어진다는 것을 전제로 한다. 즉 각 개별적인 갱신 $s \mapsto u$의 관계를 단순히 훈련 예제로 활용하며, 가치 예측에 수행되는 함수 근사를 가치 함수로 간주한다.

지도학습 방법은 인공 신경망, 결정트리, 다양한 종류의 다변수 회귀<sup>multivariate regression</sup>이 있고, 고정된 훈련 데이터 집합을 여러번의 훈련 과정에서 활용한다.

하지만 강화학습에서 사용하는 지도학습 방법은 학습자가 환경이나 환경 모델과 상호작용하며 온라인으로 학습할 수 있는 능력이 있어야 하고, 시간에 따라 변하는 비정상<sup>nonstationary</sup> 목표 함수를 다룰 수 있는 함수 근사 방법이 필요하다.

예를 들어 GPI를 기반으로 제어하는 방법에서 $\pi$가 변화하는 동안 $q_\pi$를 학습할 때, 정책의 변화가 없더라도 훈련 예제의 목표 가치가 부트스트랩 방법($\text{DP}$, $\text{TD}$)으로 생성되었을 경우 비정상적<sup>nonstationary</sup>이므로 비정상성을 다룰 수 없는 방법은 강화학습에 적용할 수 없다.
